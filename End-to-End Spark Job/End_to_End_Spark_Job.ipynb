{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HBLPVDx-q5mj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamPulse-MerchPipeline\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "num_orders = 300000\n",
        "\n",
        "categories = [\"T-Shirts\", \"Vinyl Records\", \"Posters\", \"Hoodies\", \"Stickers\", \"Accessories\"]\n",
        "regions = [\"North America\", \"Europe\", \"Asia Pacific\", \"Latin America\"]\n",
        "payment_methods = [\"credit_card\", \"debit_card\", \"paypal\", \"apple_pay\", \"gift_card\"]\n",
        "statuses = [\"completed\", \"pending\", \"cancelled\", \"refunded\", \"failed\"]\n",
        "\n",
        "prices = {\n",
        "    \"T-Shirts\": (15.99, 39.99),\n",
        "    \"Vinyl Records\": (19.99, 49.99),\n",
        "    \"Posters\": (9.99, 29.99),\n",
        "    \"Hoodies\": (39.99, 79.99),\n",
        "    \"Stickers\": (2.99, 9.99),\n",
        "    \"Accessories\": (5.99, 24.99),\n",
        "}\n",
        "\n",
        "raw_data = []\n",
        "base_date = datetime(2025, 1, 1)\n",
        "\n",
        "for i in range(num_orders):\n",
        "    order_date = base_date + timedelta(days=random.randint(0, 179))\n",
        "    category = random.choice(categories)\n",
        "    price_range = prices[category]\n",
        "    unit_price = __builtins__.round(random.uniform(*price_range), 2)\n",
        "    quantity = random.choices([1, 2, 3, 4, 5], weights=[50, 25, 15, 7, 3])[0]\n",
        "    status = random.choices(statuses, weights=[70, 10, 10, 7, 3])[0]\n",
        "\n",
        "    discount = 0.0\n",
        "    if random.random() < 0.3:\n",
        "        discount = random.choice([0.05, 0.10, 0.15, 0.20, 0.25])\n",
        "\n",
        "    shipping = __builtins__.round(random.uniform(2.99, 12.99), 2) if unit_price * quantity > 10 else 0.0\n",
        "\n",
        "    row = (\n",
        "        f\"ORD-{i+1:07d}\",\n",
        "        f\"CUST-{random.randint(1, 80000):06d}\",\n",
        "        category,\n",
        "        random.choice(regions),\n",
        "        str(unit_price),\n",
        "        str(quantity),\n",
        "        str(discount),\n",
        "        str(shipping),\n",
        "        random.choice(payment_methods),\n",
        "        status,\n",
        "        order_date.strftime(\"%Y-%m-%d\"),\n",
        "        f\"ART-{random.randint(1, 5000):05d}\",\n",
        "    )\n",
        "\n",
        "    if random.random() < 0.005:\n",
        "        row = tuple(\"\" if j == 4 else v for j, v in enumerate(row))\n",
        "    if random.random() < 0.003:\n",
        "        row = tuple(\"BAD_DATE\" if j == 10 else v for j, v in enumerate(row))\n",
        "\n",
        "    raw_data.append(row)\n",
        "\n",
        "raw_columns = [\n",
        "    \"order_id\", \"customer_id\", \"category\", \"region\", \"unit_price\",\n",
        "    \"quantity\", \"discount_pct\", \"shipping_cost\", \"payment_method\",\n",
        "    \"status\", \"order_date\", \"artist_id\"\n",
        "]\n",
        "\n",
        "df_raw = spark.createDataFrame(raw_data, raw_columns)\n",
        "df_raw.write.csv(\"pipeline/raw_orders\", header=True, mode=\"overwrite\")\n",
        "print(f\"‚úÖ Generated {df_raw.count()} raw orders\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUQHKVBbq_DE",
        "outputId": "2ab08b4e-ad70-43e5-d204-71cb07b333e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generated 300000 raw orders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and Inspect Raw Data\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, DateType\n",
        "\n",
        "# Define schema to avoid inferSchema issues with malformed numeric data\n",
        "# Read all columns as StringType initially\n",
        "schema = StructType([\n",
        "    StructField(\"order_id\", StringType(), True),\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"category\", StringType(), True),\n",
        "    StructField(\"region\", StringType(), True),\n",
        "    StructField(\"unit_price\", StringType(), True),\n",
        "    StructField(\"quantity\", StringType(), True),\n",
        "    StructField(\"discount_pct\", StringType(), True),\n",
        "    StructField(\"shipping_cost\", StringType(), True),\n",
        "    StructField(\"payment_method\", StringType(), True),\n",
        "    StructField(\"status\", StringType(), True),\n",
        "    StructField(\"order_date\", StringType(), True),\n",
        "    StructField(\"artist_id\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read CSV with defined schema\n",
        "df = spark.read.csv(\"pipeline/raw_orders\", header=True, schema=schema)\n",
        "\n",
        "# Clean and cast columns to their correct types, handling malformed data\n",
        "df = df.withColumn(\"unit_price\", when(col(\"unit_price\") == \"\", None).otherwise(col(\"unit_price\")).cast(DoubleType())) \\\n",
        "       .withColumn(\"quantity\", when(col(\"quantity\") == \"\", None).otherwise(col(\"quantity\")).cast(IntegerType())) \\\n",
        "       .withColumn(\"discount_pct\", when(col(\"discount_pct\") == \"\", None).otherwise(col(\"discount_pct\")).cast(DoubleType())) \\\n",
        "       .withColumn(\"shipping_cost\", when(col(\"shipping_cost\") == \"\", None).otherwise(col(\"shipping_cost\")).cast(DoubleType())) \\\n",
        "       .withColumn(\"order_date\", when(col(\"order_date\") == \"BAD_DATE\", None).otherwise(col(\"order_date\")).cast(DateType()))\n",
        "\n",
        "df.show(10)\n",
        "df.printSchema()\n",
        "print(f\"Total rows: {df.count()}\")\n",
        "\n",
        "print(\"\\n--- Null/Empty Counts ---\")\n",
        "# Now that types are correct and empty strings/bad dates are converted to null, check for actual nulls\n",
        "for col_name in df.columns:\n",
        "    null_count = df.filter(\n",
        "        col(col_name).isNull()\n",
        "    ).count()\n",
        "    if null_count > 0:\n",
        "        print(f\"  {col_name}: {null_count} nulls\")\n",
        "\n",
        "print(f\"\\nBad dates: {df.filter(col('order_date').isNull()).count()}\") # 'BAD_DATE' are now null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNrkQbJcrcJ5",
        "outputId": "b58d4d2c-4359-4664-c5ad-8a5fcb7e54ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-------------+-------------+----------+--------+------------+-------------+--------------+---------+----------+---------+\n",
            "|   order_id|customer_id|     category|       region|unit_price|quantity|discount_pct|shipping_cost|payment_method|   status|order_date|artist_id|\n",
            "+-----------+-----------+-------------+-------------+----------+--------+------------+-------------+--------------+---------+----------+---------+\n",
            "|ORD-0149505|CUST-058620|     Stickers| Asia Pacific|      3.89|       1|         0.2|          0.0|   credit_card|completed|2025-06-20|ART-04754|\n",
            "|ORD-0149506|CUST-067925|  Accessories|Latin America|     12.07|       2|        0.15|        11.38|     apple_pay|  pending|2025-02-26|ART-00387|\n",
            "|ORD-0149507|CUST-006271|     Stickers|North America|      3.28|       3|         0.0|          0.0|     apple_pay|  pending|2025-06-03|ART-02345|\n",
            "|ORD-0149508|CUST-033067|      Hoodies|North America|     78.84|       1|        0.05|         4.94|   credit_card|completed|2025-04-17|ART-02851|\n",
            "|ORD-0149509|CUST-043094|     T-Shirts|       Europe|      25.4|       1|         0.0|         5.33|   credit_card|completed|2025-05-12|ART-02707|\n",
            "|ORD-0149510|CUST-078989|     T-Shirts|Latin America|     36.49|       1|         0.0|        12.24|    debit_card|completed|2025-03-26|ART-03350|\n",
            "|ORD-0149511|CUST-060556|Vinyl Records|North America|     43.98|       1|         0.0|         6.03|   credit_card|completed|2025-02-04|ART-00560|\n",
            "|ORD-0149512|CUST-024621|  Accessories|Latin America|      9.24|       5|         0.0|         8.65|     apple_pay|completed|2025-04-12|ART-04797|\n",
            "|ORD-0149513|CUST-016182|      Hoodies|Latin America|     47.41|       2|        0.05|         8.78|        paypal|completed|2025-01-15|ART-01332|\n",
            "|ORD-0149514|CUST-003230|      Hoodies|North America|     64.01|       1|         0.2|         4.97|   credit_card|completed|2025-03-15|ART-02357|\n",
            "+-----------+-----------+-------------+-------------+----------+--------+------------+-------------+--------------+---------+----------+---------+\n",
            "only showing top 10 rows\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- unit_price: double (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- discount_pct: double (nullable = true)\n",
            " |-- shipping_cost: double (nullable = true)\n",
            " |-- payment_method: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- artist_id: string (nullable = true)\n",
            "\n",
            "Total rows: 300000\n",
            "\n",
            "--- Null/Empty Counts ---\n",
            "  unit_price: 1529 nulls\n",
            "  order_date: 883 nulls\n",
            "\n",
            "Bad dates: 883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and Transform\n",
        "\n",
        "order_schema = StructType([\n",
        "    StructField(\"order_id\", StringType(), False),\n",
        "    StructField(\"customer_id\", StringType(), False),\n",
        "    StructField(\"category\", StringType(), True),\n",
        "    StructField(\"region\", StringType(), True),\n",
        "    StructField(\"unit_price\", DoubleType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"discount_pct\", DoubleType(), True),\n",
        "    StructField(\"shipping_cost\", DoubleType(), True),\n",
        "    StructField(\"payment_method\", StringType(), True),\n",
        "    StructField(\"status\", StringType(), True),\n",
        "    StructField(\"order_date\", StringType(), True),\n",
        "    StructField(\"artist_id\", StringType(), True),\n",
        "])\n",
        "\n",
        "df = spark.read.csv(\"pipeline/raw_orders\", header=True, schema=order_schema)\n"
      ],
      "metadata": {
        "id": "mPMhh3eJrcHC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove invalid records\n",
        "\n",
        "df_valid = df \\\n",
        "    .filter(col(\"order_id\").isNotNull()) \\\n",
        "    .filter(col(\"unit_price\").isNotNull() & (col(\"unit_price\") > 0)) \\\n",
        "    .filter(col(\"order_date\") != \"BAD_DATE\") \\\n",
        "    .filter(col(\"order_date\").isNotNull())\n",
        "\n",
        "removed = df.count() - df_valid.count()\n",
        "print(f\"Removed {removed} invalid records ({removed/df.count()*100:.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX5KCpYFrcD9",
        "outputId": "9d4d142f-d26a-4c05-9b80-e1f21f073011"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 2410 invalid records (0.8%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cast and enrich\n",
        "\n",
        "df_enriched = df_valid \\\n",
        "    .withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\")) \\\n",
        "    .withColumn(\"discount_pct\", coalesce(col(\"discount_pct\"), lit(0.0))) \\\n",
        "    .withColumn(\"shipping_cost\", coalesce(col(\"shipping_cost\"), lit(0.0))) \\\n",
        "    .withColumn(\"subtotal\", round(col(\"unit_price\") * col(\"quantity\"), 2)) \\\n",
        "    .withColumn(\"discount_amount\", round(col(\"subtotal\") * col(\"discount_pct\"), 2)) \\\n",
        "    .withColumn(\"total_amount\", round(\n",
        "        col(\"subtotal\") - col(\"discount_amount\") + col(\"shipping_cost\"), 2\n",
        "    )) \\\n",
        "    .withColumn(\"year\", year(col(\"order_date\"))) \\\n",
        "    .withColumn(\"month\", month(col(\"order_date\"))) \\\n",
        "    .withColumn(\"day_of_week\", dayofweek(col(\"order_date\"))) \\\n",
        "    .withColumn(\"is_weekend\", when(\n",
        "        dayofweek(col(\"order_date\")).isin(1, 7), True\n",
        "    ).otherwise(False))\n",
        "\n",
        "df_enriched.show(5)\n",
        "df_enriched.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsqoT9wDsCZp",
        "outputId": "4d2cfa61-e060-4654-8ad5-6865a257a0d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-------------+----------+--------+------------+-------------+--------------+---------+----------+---------+--------+---------------+------------+----+-----+-----------+----------+\n",
            "|   order_id|customer_id|   category|       region|unit_price|quantity|discount_pct|shipping_cost|payment_method|   status|order_date|artist_id|subtotal|discount_amount|total_amount|year|month|day_of_week|is_weekend|\n",
            "+-----------+-----------+-----------+-------------+----------+--------+------------+-------------+--------------+---------+----------+---------+--------+---------------+------------+----+-----+-----------+----------+\n",
            "|ORD-0149505|CUST-058620|   Stickers| Asia Pacific|      3.89|       1|         0.2|          0.0|   credit_card|completed|2025-06-20|ART-04754|    3.89|           0.78|        3.11|2025|    6|          6|     false|\n",
            "|ORD-0149506|CUST-067925|Accessories|Latin America|     12.07|       2|        0.15|        11.38|     apple_pay|  pending|2025-02-26|ART-00387|   24.14|           3.62|        31.9|2025|    2|          4|     false|\n",
            "|ORD-0149507|CUST-006271|   Stickers|North America|      3.28|       3|         0.0|          0.0|     apple_pay|  pending|2025-06-03|ART-02345|    9.84|            0.0|        9.84|2025|    6|          3|     false|\n",
            "|ORD-0149508|CUST-033067|    Hoodies|North America|     78.84|       1|        0.05|         4.94|   credit_card|completed|2025-04-17|ART-02851|   78.84|           3.94|       79.84|2025|    4|          5|     false|\n",
            "|ORD-0149509|CUST-043094|   T-Shirts|       Europe|      25.4|       1|         0.0|         5.33|   credit_card|completed|2025-05-12|ART-02707|    25.4|            0.0|       30.73|2025|    5|          2|     false|\n",
            "+-----------+-----------+-----------+-------------+----------+--------+------------+-------------+--------------+---------+----------+---------+--------+---------------+------------+----+-----+-----------+----------+\n",
            "only showing top 5 rows\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- unit_price: double (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- discount_pct: double (nullable = false)\n",
            " |-- shipping_cost: double (nullable = false)\n",
            " |-- payment_method: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- artist_id: string (nullable = true)\n",
            " |-- subtotal: double (nullable = true)\n",
            " |-- discount_amount: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- day_of_week: integer (nullable = true)\n",
            " |-- is_weekend: boolean (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Curated Dataset\n",
        "\n",
        "df_enriched \\\n",
        "    .coalesce(4) \\\n",
        "    .write.parquet(\n",
        "        \"pipeline/curated_orders\",\n",
        "        mode=\"overwrite\",\n",
        "        partitionBy=[\"year\", \"month\"],\n",
        "        compression=\"snappy\"\n",
        "    )\n",
        "\n",
        "curated = spark.read.parquet(\"pipeline/curated_orders\")\n",
        "print(f\"‚úÖ Curated dataset: {curated.count()} rows, {len(curated.columns)} columns\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbD934JTsCWm",
        "outputId": "b313cf98-ee31-4f9d-bf94-f634ffd9525a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Curated dataset: 297590 rows, 19 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the partition structure:\n",
        "\n",
        "import os\n",
        "\n",
        "def show_tree(path, prefix=\"\", max_depth=3, depth=0):\n",
        "    if depth >= max_depth:\n",
        "        return\n",
        "    items = sorted(os.listdir(path))\n",
        "    dirs = [i for i in items if os.path.isdir(os.path.join(path, i)) and not i.startswith(\"_\")]\n",
        "    files = [i for i in items if not os.path.isdir(os.path.join(path, i)) and i.endswith(\".parquet\")]\n",
        "    for d in dirs:\n",
        "        print(f\"{prefix}üìÅ {d}/\")\n",
        "        show_tree(os.path.join(path, d), prefix + \"  \", max_depth, depth + 1)\n",
        "    for f in files:\n",
        "        size = os.path.getsize(os.path.join(path, f))\n",
        "        print(f\"{prefix}üìÑ {f} ({size/1024:.0f} KB)\")\n",
        "\n",
        "show_tree(\"pipeline/curated_orders\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKdLlVtLsCTd",
        "outputId": "08f1392e-6b90-46ee-e515-68594a964ba5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ year=2025/\n",
            "  üìÅ month=1/\n",
            "    üìÑ part-00000-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (752 KB)\n",
            "    üìÑ part-00001-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (747 KB)\n",
            "  üìÅ month=2/\n",
            "    üìÑ part-00000-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (691 KB)\n",
            "    üìÑ part-00001-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (689 KB)\n",
            "  üìÅ month=3/\n",
            "    üìÑ part-00000-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (759 KB)\n",
            "    üìÑ part-00001-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (751 KB)\n",
            "  üìÅ month=4/\n",
            "    üìÑ part-00000-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (727 KB)\n",
            "    üìÑ part-00001-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (729 KB)\n",
            "  üìÅ month=5/\n",
            "    üìÑ part-00000-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (752 KB)\n",
            "    üìÑ part-00001-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (746 KB)\n",
            "  üìÅ month=6/\n",
            "    üìÑ part-00000-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (716 KB)\n",
            "    üìÑ part-00001-30d92c80-cf1c-4b2b-aef3-0afa5e7b8288.c000.snappy.parquet (709 KB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, sum, avg, countDistinct\n",
        "\n",
        "# Build Aggregated Summary Tables\n",
        "daily_revenue = curated \\\n",
        "    .filter(col(\"status\") == \"completed\") \\\n",
        "    .groupBy(\"order_date\") \\\n",
        "    .agg(\n",
        "        count(\"order_id\").alias(\"total_orders\"),\n",
        "        sum(\"total_amount\").alias(\"total_revenue\"),\n",
        "        avg(\"total_amount\").alias(\"avg_order_value\"),\n",
        "        countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
        "    ) \\\n",
        "    .orderBy(\"order_date\")\n",
        "\n",
        "daily_revenue.show(10)\n",
        "\n",
        "daily_revenue.coalesce(1) \\\n",
        "    .write.parquet(\"pipeline/summary/daily_revenue\", mode=\"overwrite\")\n",
        "print(\"‚úÖ Daily revenue summary written\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8-Hd4PAsUlN",
        "outputId": "0f069860-3f60-4ed3-910a-7c3cea32f2c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+------------------+------------------+----------------+\n",
            "|order_date|total_orders|     total_revenue|   avg_order_value|unique_customers|\n",
            "+----------+------------+------------------+------------------+----------------+\n",
            "|2025-01-01|        1179| 65883.99999999997|55.881255301102605|            1164|\n",
            "|2025-01-02|        1156|63688.399999999994|55.093771626297574|            1149|\n",
            "|2025-01-03|        1170| 67130.27000000006|  57.3762991452992|            1163|\n",
            "|2025-01-04|        1174| 69704.56000000003|59.373560477001725|            1162|\n",
            "|2025-01-05|        1157| 65451.66999999999| 56.57015557476231|            1152|\n",
            "|2025-01-06|        1182|64904.819999999934| 54.91101522842634|            1172|\n",
            "|2025-01-07|        1206| 68506.16000000006|  56.8044444444445|            1197|\n",
            "|2025-01-08|        1125| 65306.56999999999|58.050284444444436|            1115|\n",
            "|2025-01-09|        1185| 66108.00000000003|55.787341772151926|            1175|\n",
            "|2025-01-10|        1127| 63974.19000000002|56.765031055900636|            1121|\n",
            "+----------+------------+------------------+------------------+----------------+\n",
            "only showing top 10 rows\n",
            "‚úÖ Daily revenue summary written\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, sum, avg, dense_rank, desc\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Category Performance\n",
        "category_perf = curated \\\n",
        "    .filter(col(\"status\") == \"completed\") \\\n",
        "    .groupBy(\"category\") \\\n",
        "    .agg(\n",
        "        count(\"order_id\").alias(\"total_orders\"),\n",
        "        sum(\"total_amount\").alias(\"total_revenue\"),\n",
        "        avg(\"total_amount\").alias(\"avg_order_value\"),\n",
        "        avg(\"discount_pct\").alias(\"avg_discount\"),\n",
        "        sum(\"quantity\").alias(\"total_units_sold\")\n",
        "    ) \\\n",
        "    .withColumn(\"revenue_rank\", dense_rank().over(\n",
        "        Window.orderBy(desc(\"total_revenue\"))\n",
        "    )) \\\n",
        "    .orderBy(\"revenue_rank\")\n",
        "\n",
        "category_perf.show()\n",
        "\n",
        "category_perf.coalesce(1) \\\n",
        "    .write.parquet(\"pipeline/summary/category_performance\", mode=\"overwrite\")\n",
        "print(\"‚úÖ Category performance summary written\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tifV3vssrZA",
        "outputId": "16f19180-d5bd-4ce1-c284-3ec01bef254a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------+------------------+------------------+--------------------+----------------+------------+\n",
            "|     category|total_orders|     total_revenue|   avg_order_value|        avg_discount|total_units_sold|revenue_rank|\n",
            "+-------------+------------+------------------+------------------+--------------------+----------------+------------+\n",
            "|      Hoodies|       34477| 3988137.040000002|115.67529193375299| 0.04563042028018676|           64930|           1|\n",
            "|Vinyl Records|       34992|2475490.0699999915| 70.74445787608572| 0.04515032007315957|           65847|           2|\n",
            "|     T-Shirts|       34724| 2025033.519999999|58.317979495449805|0.045291441078216754|           65387|           3|\n",
            "|      Posters|       34764|1520636.3200000052| 43.74169600736409| 0.04502214934990221|           65106|           4|\n",
            "|  Accessories|       34838|1221213.2199999988| 35.05405649003958|0.044976749526379256|           65602|           5|\n",
            "|     Stickers|       34770| 521870.5699999997| 15.00921972965199| 0.04531780270348001|           65431|           6|\n",
            "+-------------+------------+------------------+------------------+--------------------+----------------+------------+\n",
            "\n",
            "‚úÖ Category performance summary written\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Regional Trends\n",
        "regional = curated \\\n",
        "    .filter(col(\"status\") == \"completed\") \\\n",
        "    .groupBy(\"region\", \"month\") \\\n",
        "    .agg(\n",
        "        count(\"order_id\").alias(\"total_orders\"),\n",
        "        sum(\"total_amount\").alias(\"total_revenue\"),\n",
        "        countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
        "    ) \\\n",
        "    .orderBy(\"region\", \"month\")\n",
        "\n",
        "regional.show(20)\n",
        "\n",
        "regional.coalesce(1) \\\n",
        "    .write.parquet(\"pipeline/summary/regional_trends\", mode=\"overwrite\")\n",
        "print(\"‚úÖ Regional trends summary written\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udfijPlgsrVv",
        "outputId": "4b792473-b8f4-4b9f-b15c-8b75dd09f17b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+------------+------------------+----------------+\n",
            "|       region|month|total_orders|     total_revenue|unique_customers|\n",
            "+-------------+-----+------------+------------------+----------------+\n",
            "| Asia Pacific|    1|        8868| 497060.9899999998|            8395|\n",
            "| Asia Pacific|    2|        8055| 453786.0000000006|            7681|\n",
            "| Asia Pacific|    3|        8918|497517.07000000076|            8452|\n",
            "| Asia Pacific|    4|        8650|486337.20000000065|            8235|\n",
            "| Asia Pacific|    5|        8894| 500368.5399999996|            8389|\n",
            "| Asia Pacific|    6|        8377|469133.65000000026|            7945|\n",
            "|       Europe|    1|        9060|509821.58999999927|            8561|\n",
            "|       Europe|    2|        8204| 456793.9799999998|            7813|\n",
            "|       Europe|    3|        8977|497563.55000000016|            8482|\n",
            "|       Europe|    4|        8723| 493569.8400000009|            8243|\n",
            "|       Europe|    5|        9010|506017.30999999953|            8505|\n",
            "|       Europe|    6|        8447|478958.34999999974|            8014|\n",
            "|Latin America|    1|        8967|506792.96000000037|            8489|\n",
            "|Latin America|    2|        8151|461750.16999999975|            7769|\n",
            "|Latin America|    3|        8939|507210.97000000044|            8474|\n",
            "|Latin America|    4|        8680| 487369.1600000008|            8242|\n",
            "|Latin America|    5|        8966| 514051.2999999998|            8443|\n",
            "|Latin America|    6|        8486| 477476.6400000001|            8037|\n",
            "|North America|    1|        8882|502691.04999999976|            8403|\n",
            "|North America|    2|        8059| 457559.6999999988|            7670|\n",
            "+-------------+-----+------------+------------------+----------------+\n",
            "only showing top 20 rows\n",
            "‚úÖ Regional trends summary written\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Payment Method Analysis\n",
        "payment_analysis = curated \\\n",
        "    .filter(col(\"status\") == \"completed\") \\\n",
        "    .groupBy(\"payment_method\") \\\n",
        "    .agg(\n",
        "        count(\"order_id\").alias(\"total_orders\"),\n",
        "        sum(\"total_amount\").alias(\"total_revenue\"),\n",
        "        avg(\"total_amount\").alias(\"avg_order_value\"),\n",
        "    ) \\\n",
        "    .withColumn(\"pct_of_orders\", round(\n",
        "        col(\"total_orders\") / sum(\"total_orders\").over(Window.partitionBy()), 4\n",
        "    )) \\\n",
        "    .orderBy(desc(\"total_orders\"))\n",
        "\n",
        "payment_analysis.show()\n",
        "\n",
        "payment_analysis.coalesce(1) \\\n",
        "    .write.parquet(\"pipeline/summary/payment_analysis\", mode=\"overwrite\")\n",
        "print(\"‚úÖ Payment analysis summary written\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSBHG58gsrSs",
        "outputId": "f560a574-3443-477b-b495-905e727e8d58"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+------------------+------------------+-------------+\n",
            "|payment_method|total_orders|     total_revenue|   avg_order_value|pct_of_orders|\n",
            "+--------------+------------+------------------+------------------+-------------+\n",
            "|        paypal|       42337|2380316.7599999923| 56.22308524458494|        0.203|\n",
            "|     apple_pay|       41697|  2360488.59000001| 56.61051370602226|       0.1999|\n",
            "|   credit_card|       41650| 2346339.889999993|56.334691236494436|       0.1997|\n",
            "|     gift_card|       41454|2336333.4800000014| 56.35966324118303|       0.1988|\n",
            "|    debit_card|       41427| 2328902.019999999|56.217008714123615|       0.1986|\n",
            "+--------------+------------+------------------+------------------+-------------+\n",
            "\n",
            "‚úÖ Payment analysis summary written\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline Validation\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PIPELINE VALIDATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "raw_count = spark.read.csv(\"pipeline/raw_orders\", header=True).count()\n",
        "curated_count = spark.read.parquet(\"pipeline/curated_orders\").count()\n",
        "daily_count = spark.read.parquet(\"pipeline/summary/daily_revenue\").count()\n",
        "cat_count = spark.read.parquet(\"pipeline/summary/category_performance\").count()\n",
        "\n",
        "print(f\"\\n1. Row Counts:\")\n",
        "print(f\"   Raw orders:          {raw_count}\")\n",
        "print(f\"   Curated orders:      {curated_count}\")\n",
        "print(f\"   Records removed:     {raw_count - curated_count} ({(raw_count-curated_count)/raw_count*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n2. Summary Tables:\")\n",
        "print(f\"   Daily revenue:       {daily_count} days\")\n",
        "print(f\"   Category perf:       {cat_count} categories\")\n",
        "\n",
        "print(f\"\\n3. Data Quality Checks:\")\n",
        "curated_df = spark.read.parquet(\"pipeline/curated_orders\")\n",
        "null_ids = curated_df.filter(col(\"order_id\").isNull()).count()\n",
        "null_amounts = curated_df.filter(col(\"total_amount\").isNull()).count()\n",
        "neg_amounts = curated_df.filter(col(\"total_amount\") < 0).count()\n",
        "print(f\"   Null order_ids:      {null_ids} {'‚úÖ' if null_ids == 0 else '‚ùå'}\")\n",
        "print(f\"   Null total_amounts:  {null_amounts} {'‚úÖ' if null_amounts == 0 else '‚ùå'}\")\n",
        "print(f\"   Negative amounts:    {neg_amounts} {'‚úÖ' if neg_amounts == 0 else '‚ùå'}\")\n",
        "\n",
        "print(f\"\\n4. Schema Validation:\")\n",
        "expected_cols = [\n",
        "    \"order_id\", \"customer_id\", \"category\", \"region\", \"unit_price\",\n",
        "    \"quantity\", \"discount_pct\", \"shipping_cost\", \"payment_method\",\n",
        "    \"status\", \"order_date\", \"artist_id\", \"subtotal\", \"discount_amount\",\n",
        "    \"total_amount\", \"year\", \"month\", \"day_of_week\", \"is_weekend\"\n",
        "]\n",
        "actual_cols = curated_df.columns\n",
        "missing = set(expected_cols) - set(actual_cols)\n",
        "extra = set(actual_cols) - set(expected_cols)\n",
        "print(f\"   Expected columns:    {len(expected_cols)}\")\n",
        "print(f\"   Actual columns:      {len(actual_cols)}\")\n",
        "print(f\"   Missing:             {missing if missing else '‚úÖ None'}\")\n",
        "print(f\"   Extra:               {extra if extra else 'None'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PIPELINE COMPLETE ‚úÖ\")\n",
        "print(\"=\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al3axVywsrPk",
        "outputId": "966a289e-3239-4bd7-9c85-758db9d40f95"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PIPELINE VALIDATION REPORT\n",
            "============================================================\n",
            "\n",
            "1. Row Counts:\n",
            "   Raw orders:          300000\n",
            "   Curated orders:      297590\n",
            "   Records removed:     2410 (0.8%)\n",
            "\n",
            "2. Summary Tables:\n",
            "   Daily revenue:       180 days\n",
            "   Category perf:       6 categories\n",
            "\n",
            "3. Data Quality Checks:\n",
            "   Null order_ids:      0 ‚úÖ\n",
            "   Null total_amounts:  0 ‚úÖ\n",
            "   Negative amounts:    0 ‚úÖ\n",
            "\n",
            "4. Schema Validation:\n",
            "   Expected columns:    19\n",
            "   Actual columns:      19\n",
            "   Missing:             ‚úÖ None\n",
            "   Extra:               None\n",
            "\n",
            "============================================================\n",
            "PIPELINE COMPLETE ‚úÖ\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What data quality issues did you find?\n",
        "\n",
        "Missing values in unit_price (Nulls).\n",
        "\n",
        "Malformed strings in order_date (\"BAD_DATE\").\n",
        "\n",
        "Varying formats (converting strings to actual Date objects).\n",
        "\n",
        "How did your cleaning handle them?\n",
        "\n",
        "We used an Internal Filter to drop rows where unit_price was missing or the date was invalid, ensuring the final analytics weren't skewed.\n",
        "\n",
        "We used to_date to standardize the strings into a proper temporal format for time-series analysis.\n",
        "\n",
        "What would you change for production?\n",
        "\n",
        "Partitioning: I would add partitionBy(\"year\", \"month\") when writing to disk to avoid the performance warning you saw.\n",
        "\n",
        "Schema Enforcement: Instead of letting Spark guess, I'd use a strict StructType schema to prevent the pipeline from running if the source data format changes\n"
      ],
      "metadata": {
        "id": "ctDumtZ8tF1J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tma3XvvtNrl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}