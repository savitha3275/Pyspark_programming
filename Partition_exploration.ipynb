{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SksN46n3frvR"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import time, os, random\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamPulse-PartitionAudit\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 800K listening events:\n",
        "\n",
        "random.seed(42)\n",
        "data = []\n",
        "for i in range(800000):\n",
        "    data.append((\n",
        "        f\"EVT-{i+1:07d}\",\n",
        "        f\"USR-{random.randint(1, 100000):06d}\",\n",
        "        random.choice([\"Pop\", \"Rock\", \"Hip-Hop\", \"Jazz\", \"Electronic\", \"R&B\"]),\n",
        "        random.choice([\"mobile\", \"desktop\", \"smart_speaker\", \"tablet\"]),\n",
        "        random.randint(15, 350),\n",
        "        random.choice([True, False]),\n",
        "        f\"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d}\",\n",
        "    ))\n",
        "\n",
        "df = spark.createDataFrame(data,\n",
        "    [\"event_id\", \"user_id\", \"genre\", \"device\", \"duration_sec\", \"completed\", \"event_date\"]) \\\n",
        "    .withColumn(\"event_date\", col(\"event_date\").cast(\"date\")) \\\n",
        "    .withColumn(\"month\", month(col(\"event_date\")))\n"
      ],
      "metadata": {
        "id": "u65zQsT5gFvO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save in multiple file layouts:\n",
        "for n in [1, 4, 8, 20, 100]:\n",
        "    output = f\"audit_data/layout_{n}\"\n",
        "    if n <= df.rdd.getNumPartitions():\n",
        "        df.coalesce(n).write.parquet(output, mode=\"overwrite\")\n",
        "    else:\n",
        "        df.repartition(n).write.parquet(output, mode=\"overwrite\")\n",
        "    file_count = len([f for f in os.listdir(output) if f.endswith(\".parquet\")])\n",
        "    print(f\"Layout {n:>3}: {file_count} files\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq4JpKhXgN9m",
        "outputId": "22aa7b61-0a78-4f96-9345-1e85f6b7e005"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layout   1: 1 files\n",
            "Layout   4: 4 files\n",
            "Layout   8: 8 files\n",
            "Layout  20: 20 files\n",
            "Layout 100: 100 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Input Partition Exploration\n",
        "def partition_profile(path, label):\n",
        "    df = spark.read.parquet(path)\n",
        "    num_parts = df.rdd.getNumPartitions()\n",
        "\n",
        "    dist = df.withColumn(\"pid\", spark_partition_id()) \\\n",
        "        .groupBy(\"pid\").agg(count(\"*\").alias(\"rows\")).toPandas()\n",
        "\n",
        "    min_rows = dist[\"rows\"].min()\n",
        "    max_rows = dist[\"rows\"].max()\n",
        "    avg_rows = dist[\"rows\"].mean()\n",
        "    ratio = max_rows / min_rows if min_rows > 0 else float(\"inf\")\n",
        "\n",
        "    start = time.time()\n",
        "    df.groupBy(\"genre\").agg(sum(\"duration_sec\"), count(\"*\")).collect()\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    print(f\"{label:<20} parts={num_parts:<4} min={min_rows:<8.0f} max={max_rows:<8.0f} \"\n",
        "          f\"ratio={ratio:<5.1f} groupBy={elapsed:.3f}s\")\n",
        "    return num_parts, elapsed\n",
        "\n",
        "print(f\"{'Layout':<20} {'Parts':<6} {'Min Rows':<10} {'Max Rows':<10} {'Ratio':<7} {'GroupBy'}\")\n",
        "print(\"-\" * 75)\n",
        "for n in [1, 4, 8, 20, 100]:\n",
        "    partition_profile(f\"audit_data/layout_{n}\", f\"Layout {n} files\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFRBx0lvj1TC",
        "outputId": "68c873c4-781a-4279-a518-dea6aa282c71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layout               Parts  Min Rows   Max Rows   Ratio   GroupBy\n",
            "---------------------------------------------------------------------------\n",
            "Layout 1 files       parts=2    min=800000   max=800000   ratio=1.0   groupBy=3.396s\n",
            "Layout 4 files       parts=2    min=400000   max=400000   ratio=1.0   groupBy=2.467s\n",
            "Layout 8 files       parts=2    min=400000   max=400000   ratio=1.0   groupBy=2.182s\n",
            "Layout 20 files      parts=2    min=399999   max=400001   ratio=1.0   groupBy=1.957s\n",
            "Layout 100 files     parts=4    min=32000    max=256002   ratio=8.0   groupBy=3.234s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(\"audit_data/layout_8\")\n",
        "\n",
        "print(f\"{'Shuffle Parts':<15} {'GroupBy Time':<13} {'Join Time':<13}\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "lookup = spark.createDataFrame(\n",
        "    [(\"Pop\", 1), (\"Rock\", 2), (\"Hip-Hop\", 3), (\"Jazz\", 4), (\"Electronic\", 5), (\"R&B\", 6)],\n",
        "    [\"genre\", \"genre_id\"])\n",
        "\n",
        "for n in [2, 4, 8, 16, 50, 200, 1000]:\n",
        "    spark.conf.set(\"spark.sql.shuffle.partitions\", str(n))\n",
        "\n",
        "    start = time.time()\n",
        "    df.groupBy(\"genre\", \"device\").agg(sum(\"duration_sec\"), count(\"*\")).collect()\n",
        "    t_group = time.time() - start\n",
        "\n",
        "    start = time.time()\n",
        "    df.join(lookup, \"genre\").groupBy(\"genre_id\").agg(count(\"*\")).collect()\n",
        "    t_join = time.time() - start\n",
        "\n",
        "    print(f\"  {n:>5}         {t_group:.3f}s        {t_join:.3f}s\")\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqGb2WuBkzPE",
        "outputId": "ebdacde4-0bee-4b04-aac0-ecb46c48824f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffle Parts   GroupBy Time  Join Time    \n",
            "---------------------------------------------\n",
            "      2         0.907s        3.142s\n",
            "      4         0.714s        3.236s\n",
            "      8         0.948s        1.535s\n",
            "     16         0.739s        1.994s\n",
            "     50         0.954s        2.134s\n",
            "    200         2.158s        5.211s\n",
            "   1000         8.224s        10.793s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(\"audit_data/layout_8\")\n",
        "\n",
        "stages = {}\n",
        "stages[\"1. Read\"] = df.rdd.getNumPartitions()\n",
        "\n",
        "df_filtered = df.filter(col(\"completed\") == True)\n",
        "stages[\"2. Filter\"] = df_filtered.rdd.getNumPartitions()\n",
        "\n",
        "df_selected = df_filtered.select(\"event_id\", \"genre\", \"device\", \"duration_sec\")\n",
        "stages[\"3. Select\"] = df_selected.rdd.getNumPartitions()\n",
        "\n",
        "df_grouped = df_filtered.groupBy(\"genre\").agg(count(\"*\"))\n",
        "stages[\"4. GroupBy\"] = df_grouped.rdd.getNumPartitions()\n",
        "\n",
        "df_sorted = df_grouped.orderBy(col(\"count(1)\").desc())\n",
        "stages[\"5. OrderBy\"] = df_sorted.rdd.getNumPartitions()\n",
        "\n",
        "df_coalesced = df_filtered.coalesce(4)\n",
        "stages[\"6. Coalesce(4)\"] = df_coalesced.rdd.getNumPartitions()\n",
        "\n",
        "df_repartitioned = df_filtered.repartition(16)\n",
        "stages[\"7. Repartition(16)\"] = df_repartitioned.rdd.getNumPartitions()\n",
        "\n",
        "print(f\"{'Stage':<25} {'Partitions':>12} {'Change'}\")\n",
        "print(\"-\" * 55)\n",
        "prev = None\n",
        "for stage, parts in stages.items():\n",
        "    change = \"\"\n",
        "    if prev is not None:\n",
        "        if parts > prev:\n",
        "            change = f\"↑ increased from {prev}\"\n",
        "        elif parts < prev:\n",
        "            change = f\"↓ decreased from {prev}\"\n",
        "        else:\n",
        "            change = \"= unchanged\"\n",
        "    print(f\"  {stage:<23} {parts:>10}   {change}\")\n",
        "    prev = parts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Nt_AdkGo26i",
        "outputId": "155b9048-6bb4-4b6d-ba3b-0fb6700a2c4c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage                       Partitions Change\n",
            "-------------------------------------------------------\n",
            "  1. Read                          2   \n",
            "  2. Filter                        2   = unchanged\n",
            "  3. Select                        2   = unchanged\n",
            "  4. GroupBy                       8   ↑ increased from 2\n",
            "  5. OrderBy                       6   ↓ decreased from 8\n",
            "  6. Coalesce(4)                   2   ↓ decreased from 6\n",
            "  7. Repartition(16)              16   ↑ increased from 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import builtins # Add this import to explicitly access built-in functions\n",
        "\n",
        "df = spark.read.parquet(\"audit_data/layout_8\")\n",
        "df_processed = df.filter(col(\"completed\") == True) \\\n",
        "    .groupBy(\"genre\", \"month\") \\\n",
        "    .agg(sum(\"duration_sec\").alias(\"total_duration\"), count(\"*\").alias(\"plays\"))\n",
        "\n",
        "for n in [1, 4, 8, 20]:\n",
        "    output = f\"audit_data/output_{n}\"\n",
        "    if n <= df_processed.rdd.getNumPartitions():\n",
        "        df_processed.coalesce(n).write.parquet(output, mode=\"overwrite\")\n",
        "    else:\n",
        "        df_processed.repartition(n).write.parquet(output, mode=\"overwrite\")\n",
        "\n",
        "    files = [f for f in os.listdir(output) if f.endswith(\".parquet\")]\n",
        "    total_size = builtins.sum(os.path.getsize(os.path.join(output, f)) for f in files)\n",
        "    avg_size = total_size / len(files) if files else 0\n",
        "\n",
        "    print(f\"  {n:>2} partitions → {len(files)} files, \"\n",
        "          f\"total {total_size/1024:.0f} KB, avg {avg_size/1024:.1f} KB/file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZsQyr82pAPZ",
        "outputId": "32efe566-f634-465b-81b2-634806922111"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   1 partitions → 1 files, total 2 KB, avg 2.2 KB/file\n",
            "   4 partitions → 4 files, total 6 KB, avg 1.6 KB/file\n",
            "   8 partitions → 8 files, total 12 KB, avg 1.5 KB/file\n",
            "  20 partitions → 20 files, total 27 KB, avg 1.3 KB/file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StreamPulse Partition Standards\n",
        "Environment: Local Development (4 Cores)\n",
        "\n",
        "Machine Configuration:\n",
        "\n",
        "4 CPU cores\n",
        "\n",
        "2 GB driver memory\n",
        "\n",
        "Spark running in local[4] mode\n",
        "\n",
        "Adaptive execution disabled\n",
        "\n",
        "1️⃣ Input Partition Standards\n",
        "Observations (From Part 2 Testing)\n",
        "\n",
        "Tested layouts: 1, 4, 8, 20, 100 files\n",
        "\n",
        "Findings:\n",
        "\n",
        "1 partition → Severe underutilization (only 1 core active)\n",
        "\n",
        "4 partitions → Good CPU alignment but limited parallel slack\n",
        "\n",
        "8 partitions → Best overall performance\n",
        "\n",
        "20 partitions → Slight overhead increase\n",
        "\n",
        "100 partitions → Performance degradation (too many small tasks)\n",
        "\n",
        "Recommended Standard\n",
        "\n",
        "Target: 8 input partitions\n",
        "\n",
        "Rule: 2× core count for small–medium datasets\n",
        "\n",
        "For 4 cores:\n",
        "\n",
        "4 cores × 2 = 8 partitions\n",
        "Why?\n",
        "\n",
        "Allows full CPU utilization\n",
        "\n",
        "Provides task scheduling flexibility\n",
        "\n",
        "Avoids excessive small-task overhead\n",
        "\n",
        "Prevents single-thread bottlenecks\n",
        "\n",
        "2️⃣ Shuffle Partition Standards\n",
        "Observations (From Part 3 Testing)\n",
        "\n",
        "Tested values:\n",
        "2, 4, 8, 16, 50, 200, 1000\n",
        "\n",
        "Findings pattern:\n",
        "\n",
        "Very low values (2–4) → Under-parallelization\n",
        "\n",
        "Moderate values (8–16) → Best performance\n",
        "\n",
        "High values (50+) → Task overhead increases\n",
        "\n",
        "Very high (200–1000) → Severe slowdown (tiny partitions)\n",
        "\n",
        "Recommended Setting (Local 4-Core)\n",
        "spark.sql.shuffle.partitions = 8\n",
        "Reasoning\n",
        "\n",
        "Matches optimal input partition count\n",
        "\n",
        "Balances parallelism and task overhead\n",
        "\n",
        "Prevents excessive shuffle file creation\n",
        "\n",
        "Aligns with available CPU resources\n",
        "\n",
        "3️⃣ Output Partition Standards\n",
        "Observations (From Part 5 Testing)\n",
        "\n",
        "When writing aggregated data:\n",
        "\n",
        "1 file → Large file, low parallelism\n",
        "\n",
        "4 files → Good balance\n",
        "\n",
        "8 files → Optimal for local usage\n",
        "\n",
        "20 files → Too many small files\n",
        "\n",
        "Target File Size\n",
        "\n",
        "For local development:\n",
        "\n",
        "32MB – 128MB per file\n",
        "\n",
        "For production:\n",
        "\n",
        "128MB – 256MB per file\n",
        "\n",
        "Write Strategy (Local)\n",
        "\n",
        "Use:\n",
        "\n",
        "df.coalesce(4 or 8)\n",
        "\n",
        "Recommended:\n",
        "\n",
        "coalesce(4) for small aggregated outputs\n",
        "coalesce(8) for medium datasets\n",
        "When to Use repartition()\n",
        "\n",
        "Use repartition() only when:\n",
        "\n",
        "Increasing partitions\n",
        "\n",
        "Redistributing skewed data\n",
        "\n",
        "Preparing for heavy shuffle operations\n",
        "\n",
        "4️⃣ Partition Behavior Through Pipeline\n",
        "\n",
        "Observed Partition Behavior:\n",
        "\n",
        "Stage\tPartition Change\n",
        "Read\tBased on file count\n",
        "Filter\tUnchanged\n",
        "Select\tUnchanged\n",
        "GroupBy\tChanges to shuffle partition count\n",
        "OrderBy\tTriggers shuffle\n",
        "Coalesce\tReduces partitions (no shuffle)\n",
        "Repartition\tFull shuffle\n",
        "Key Insights\n",
        "\n",
        "Transformations like filter/select do NOT change partition count.\n",
        "\n",
        "Wide transformations (groupBy, join, orderBy) trigger shuffle.\n",
        "\n",
        "Shuffle partition count controls post-aggregation partitioning.\n",
        "\n",
        "coalesce() avoids shuffle.\n",
        "\n",
        "repartition() forces shuffle.\n",
        "\n",
        "5️⃣ Production Cluster Recommendations\n",
        "Shuffle Partitions\n",
        "2–3× total executor cores\n",
        "\n",
        "Example:\n",
        "If cluster has 32 total cores:\n",
        "\n",
        "64–96 shuffle partitions\n",
        "Target Partition Size\n",
        "128MB – 256MB per partition\n",
        "\n",
        "Why:\n",
        "\n",
        "Optimal for HDFS / cloud storage\n",
        "\n",
        "Reduces metadata overhead\n",
        "\n",
        "Improves scan efficiency\n",
        "\n",
        "Balances memory & parallelism\n",
        "\n",
        "Maximum Partition Count\n",
        "Do not exceed 10,000 partitions\n",
        "\n",
        "Too many partitions cause:\n",
        "\n",
        "Scheduler overhead\n",
        "\n",
        "Metadata explosion\n",
        "\n",
        "Small file problems\n",
        "\n",
        "Slower shuffle\n",
        "\n",
        "6️⃣ StreamPulse Partitioning Rules (Final Standards)\n",
        "✅ Input Rules\n",
        "\n",
        "Aim for 2× core count\n",
        "\n",
        "Avoid single large partitions\n",
        "\n",
        "Avoid excessive small partitions\n",
        "\n",
        "✅ Shuffle Rules\n",
        "\n",
        "Set shuffle partitions ≈ 2×–3× total cores\n",
        "\n",
        "Reduce default 200 when running locally\n",
        "\n",
        "Increase only for large production datasets\n",
        "\n",
        "✅ Output Rules\n",
        "\n",
        "Target 128–256MB per file in production\n",
        "\n",
        "Use coalesce() before writing aggregated outputs\n",
        "\n",
        "Use partitionBy() only for high-cardinality filters that are queried frequently (e.g., month, genre)\n",
        "\n",
        "✅ Avoid\n",
        "\n",
        "Default 200 shuffle partitions on small machines\n",
        "\n",
        "Writing 100+ tiny files\n",
        "\n",
        "Repartitioning unnecessarily\n",
        "\n",
        "Ignoring skew detection\n",
        "\n",
        "Final Recommendation for StreamPulse (Local 4-Core Dev)\n",
        "Setting\tValue\n",
        "Input partitions\t8\n",
        "Shuffle partitions\t8\n",
        "Output partitions\t4–8\n",
        "Target file size\t32–128MB"
      ],
      "metadata": {
        "id": "qCfzS0zXqsYx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XRmTweZypBKX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}