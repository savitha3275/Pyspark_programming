{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L85MjP7oMFtH",
        "outputId": "ace0ae6b-723a-456a-f503-b7a9158300bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark version: 4.0.2\n",
            "App name: M16-Lab02-RDD-vs-DataFrame\n",
            "✅ SparkSession created\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"M16-Lab02-RDD-vs-DataFrame\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"App name: {spark.sparkContext.appName}\")\n",
        "print(\"✅ SparkSession created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM8sAPzXMgBo",
        "outputId": "fe59e692-77d6-42ae-a8f8-ff6f593f2f61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ CSV files created\n"
          ]
        }
      ],
      "source": [
        "# Create plays.csv\n",
        "\n",
        "plays_data = \"\"\"play_id,user_id,song_id,artist_id,played_at,duration_seconds\n",
        "P001,U101,S001,A01,2025-03-01 08:15:00,240\n",
        "P002,U102,S002,A02,2025-03-01 09:30:00,180\n",
        "P003,U101,S001,A01,2025-03-01 10:00:00,240\n",
        "P004,U103,S003,A01,2025-03-01 11:45:00,300\n",
        "P005,U102,S001,A01,2025-03-01 12:00:00,240\n",
        "P006,U104,S004,A03,2025-03-01 13:30:00,200\n",
        "P007,U101,S002,A02,2025-03-01 14:00:00,180\n",
        "P008,U105,S005,A02,2025-03-01 15:15:00,220\n",
        "P009,U103,S001,A01,2025-03-01 16:00:00,240\n",
        "P010,U104,S003,A01,2025-03-01 17:30:00,300\n",
        "P011,U102,S004,A03,2025-03-02 08:00:00,200\n",
        "P012,U101,S005,A02,2025-03-02 09:15:00,220\n",
        "P013,U105,S001,A01,2025-03-02 10:30:00,240\n",
        "P014,U103,S002,A02,2025-03-02 11:00:00,180\n",
        "P015,U104,S001,A01,2025-03-02 12:45:00,240\"\"\"\n",
        "\n",
        "with open(\"plays.csv\", \"w\") as f:\n",
        "    f.write(plays_data)\n",
        "\n",
        "# Create songs.csv\n",
        "\n",
        "songs_data = \"\"\"song_id,song_name,genre,release_year\n",
        "S001,Midnight Drive,Pop,2024\n",
        "S002,Ocean Waves,Rock,2023\n",
        "S003,City Lights,Pop,2025\n",
        "S004,Thunder Road,Rock,2024\n",
        "S005,Sunset Blvd,Jazz,2023\"\"\"\n",
        "\n",
        "with open(\"songs.csv\", \"w\") as f:\n",
        "    f.write(songs_data)\n",
        "\n",
        "print(\"✅ CSV files created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T3pjm0NMf-D",
        "outputId": "92a83b0c-67cd-4385-b4cb-fbd633363386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "PIPELINE 1: Play Count by Song (RDD)\n",
            "==================================================\n",
            "\n",
            "RDD Result — Play count by song:\n",
            "  S001: 6 plays\n",
            "  S002: 3 plays\n",
            "  S004: 2 plays\n",
            "  S003: 2 plays\n",
            "  S005: 2 plays\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"PIPELINE 1: Play Count by Song (RDD)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "rdd = sc.textFile(\"plays.csv\")\n",
        "\n",
        "header = rdd.first()\n",
        "data = rdd.filter(lambda line: line != header)\n",
        "\n",
        "parsed = data.map(lambda line: line.split(\",\"))\n",
        "\n",
        "song_plays = (\n",
        "    parsed\n",
        "    .map(lambda row: (row[2], 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .sortBy(lambda x: -x[1])\n",
        ")\n",
        "\n",
        "print(\"\\nRDD Result — Play count by song:\")\n",
        "for song_id, count in song_plays.collect():\n",
        "    print(f\"  {song_id}: {count} plays\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuYVUVJRMf6Q",
        "outputId": "f22feffb-10f5-4557-fa9b-7ee7c2537026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "PIPELINE 1: Play Count by Song (DataFrame)\n",
            "==================================================\n",
            "\n",
            "DataFrame Result — Play count by song:\n",
            "+-------+----------+\n",
            "|song_id|play_count|\n",
            "+-------+----------+\n",
            "|   S001|         6|\n",
            "|   S002|         3|\n",
            "|   S004|         2|\n",
            "|   S005|         2|\n",
            "|   S003|         2|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# dataframe version:\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"PIPELINE 1: Play Count by Song (DataFrame)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "from pyspark.sql.functions import col, count, desc\n",
        "\n",
        "df = spark.read.csv(\"plays.csv\", header=True, inferSchema=True)\n",
        "\n",
        "song_plays_df = (\n",
        "    df.groupBy(\"song_id\")\n",
        "      .agg(count(\"play_id\").alias(\"play_count\"))\n",
        "      .orderBy(desc(\"play_count\"))\n",
        ")\n",
        "\n",
        "print(\"\\nDataFrame Result — Play count by song:\")\n",
        "song_plays_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DkEzpWDNPlP",
        "outputId": "ab6860d0-0a08-460a-a27b-bbe642b41f76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verification:\n",
            "RDD result:       {'S001': 6, 'S002': 3, 'S004': 2, 'S003': 2, 'S005': 2}\n",
            "DataFrame result: {'S001': 6, 'S002': 3, 'S004': 2, 'S005': 2, 'S003': 2}\n",
            "Results match:    True\n"
          ]
        }
      ],
      "source": [
        "#Step 2c: Compare and verify\n",
        "rdd_result = dict(song_plays.collect())\n",
        "df_result = {row[\"song_id\"]: row[\"play_count\"] for row in song_plays_df.collect()}\n",
        "\n",
        "print(\"\\nVerification:\")\n",
        "print(f\"RDD result:       {rdd_result}\")\n",
        "print(f\"DataFrame result: {df_result}\")\n",
        "print(f\"Results match:    {rdd_result == df_result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TnBaUouNdoF"
      },
      "source": [
        "## Pipeline 1 — Play Count by Song\n",
        "\n",
        "| Metric | RDD Version | DataFrame Version |\n",
        "|--------|-------------|------------------|\n",
        "| Lines of code | 14 | 8 |\n",
        "| Column access | By index (row[2]) | By name (\"song_id\") |\n",
        "| CSV parsing | Manual (split(\",\")) | Automatic (header=True, inferSchema=True) |\n",
        "| Optimization | None | Catalyst optimizer |\n",
        "| Readability | Low (row[2] is unclear) | High (explicit column names) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "359RgyF2NPh4",
        "outputId": "4bfd627d-5310-468e-9000-65cb0558bbe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "PIPELINE 2: Listening Time per User (RDD)\n",
            "==================================================\n",
            "\n",
            "RDD Result — Listening time per user (minutes):\n",
            "  U101: 14.7 min\n",
            "  U104: 12.3 min\n",
            "  U103: 12.0 min\n",
            "  U102: 10.3 min\n",
            "  U105: 7.7 min\n"
          ]
        }
      ],
      "source": [
        "# Pipeline 2 — Total Listening Time per User\n",
        "\n",
        "# Step 3a: RDD Version (Legacy)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"PIPELINE 2: Listening Time per User (RDD)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "rdd = sc.textFile(\"plays.csv\")\n",
        "header = rdd.first()\n",
        "data = rdd.filter(lambda line: line != header)\n",
        "parsed = data.map(lambda line: line.split(\",\"))\n",
        "\n",
        "user_time = (\n",
        "    parsed\n",
        "    .map(lambda row: (row[1], int(row[5])))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .mapValues(lambda seconds: round(seconds / 60, 1))\n",
        "    .sortBy(lambda x: -x[1])\n",
        ")\n",
        "\n",
        "print(\"\\nRDD Result — Listening time per user (minutes):\")\n",
        "for user_id, minutes in user_time.collect():\n",
        "    print(f\"  {user_id}: {minutes} min\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGYkjswENPfD",
        "outputId": "00c4531b-13b8-4c59-a9de-1c290d377f50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "PIPELINE 2: Listening Time per User (DataFrame)\n",
            "==================================================\n",
            "\n",
            "DataFrame Result — Listening time per user (minutes):\n",
            "+-------+-------------+\n",
            "|user_id|total_minutes|\n",
            "+-------+-------------+\n",
            "|   U101|         14.7|\n",
            "|   U104|         12.3|\n",
            "|   U103|         12.0|\n",
            "|   U102|         10.3|\n",
            "|   U105|          7.7|\n",
            "+-------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 3b: DataFrame Version (Rewrite)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"PIPELINE 2: Listening Time per User (DataFrame)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "from pyspark.sql.functions import sum as spark_sum, round as spark_round\n",
        "\n",
        "df = spark.read.csv(\"plays.csv\", header=True, inferSchema=True)\n",
        "\n",
        "user_time_df = (\n",
        "    df.groupBy(\"user_id\")\n",
        "      .agg(\n",
        "          spark_round(spark_sum(\"duration_seconds\") / 60, 1).alias(\"total_minutes\")\n",
        "      )\n",
        "      .orderBy(desc(\"total_minutes\"))\n",
        ")\n",
        "\n",
        "print(\"\\nDataFrame Result — Listening time per user (minutes):\")\n",
        "user_time_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV3XpF-pNPco",
        "outputId": "f0335c03-4785-4dfd-ade9-d210e9aeace7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verification:\n",
            "RDD result:       {'U101': 14.7, 'U104': 12.3, 'U103': 12.0, 'U102': 10.3, 'U105': 7.7}\n",
            "DataFrame result: {'U101': 14.7, 'U104': 12.3, 'U103': 12.0, 'U102': 10.3, 'U105': 7.7}\n",
            "Results match:    True\n"
          ]
        }
      ],
      "source": [
        "# Step 3c: Compare and verify\n",
        "\n",
        "rdd_result = dict(user_time.collect())\n",
        "df_result = {row[\"user_id\"]: row[\"total_minutes\"] for row in user_time_df.collect()}\n",
        "\n",
        "print(\"\\nVerification:\")\n",
        "print(f\"RDD result:       {rdd_result}\")\n",
        "print(f\"DataFrame result: {df_result}\")\n",
        "print(f\"Results match:    {rdd_result == df_result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tureVKLBOrkP"
      },
      "source": [
        "## Pipeline 2 — Listening Time per User\n",
        "\n",
        "| Metric | RDD Version | DataFrame Version |\n",
        "|--------|-------------|------------------|\n",
        "| Lines of code | 15 | 9 |\n",
        "| Column access | By index (row[1], row[5]) | By name (\"user_id\", \"duration_seconds\") |\n",
        "| Type handling | Manual (int(row[5])) | Automatic (inferSchema=True) |\n",
        "| Aggregation | reduceByKey + mapValues | groupBy + agg |\n",
        "| Optimization | None | Catalyst optimizer |\n",
        "| Readability | Medium-Low (index-based access) | High (clear column operations) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5whSXQAOusI"
      },
      "source": [
        "### Analysis\n",
        "\n",
        "- The DataFrame version reduces code by approximately 40%.\n",
        "- Manual type conversion (int(row[5])) is eliminated with schema inference.\n",
        "- Aggregations are more declarative and SQL-like using groupBy and agg.\n",
        "- Column names make the logic clearer and easier to maintain.\n",
        "- Catalyst optimizer enables performance improvements not available in RDDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHdBqTIOO7L0",
        "outputId": "56fef766-a9c1-4d56-f86a-2782484f4b25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "PIPELINE 3: Top Songs with Genre (RDD)\n",
            "==================================================\n",
            "\n",
            "RDD Result — Top songs with genre:\n",
            "  Midnight Drive (Pop): 6 plays\n",
            "  Ocean Waves (Rock): 3 plays\n",
            "  Thunder Road (Rock): 2 plays\n",
            "  Sunset Blvd (Jazz): 2 plays\n",
            "  City Lights (Pop): 2 plays\n"
          ]
        }
      ],
      "source": [
        "# Pipeline 3 — Top Songs with Genre\n",
        "\n",
        "# Step 4a: RDD Version (Legacy)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"PIPELINE 3: Top Songs with Genre (RDD)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "plays_rdd = sc.textFile(\"plays.csv\")\n",
        "plays_header = plays_rdd.first()\n",
        "plays_data = plays_rdd.filter(lambda line: line != plays_header) \\\n",
        "                       .map(lambda line: line.split(\",\"))\n",
        "\n",
        "songs_rdd = sc.textFile(\"songs.csv\")\n",
        "songs_header = songs_rdd.first()\n",
        "songs_data = songs_rdd.filter(lambda line: line != songs_header) \\\n",
        "                       .map(lambda line: line.split(\",\"))\n",
        "\n",
        "plays_keyed = plays_data.map(lambda row: (row[2], 1)) \\\n",
        "                        .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "songs_keyed = songs_data.map(lambda row: (row[0], (row[1], row[2])))\n",
        "\n",
        "joined = plays_keyed.join(songs_keyed) \\\n",
        "                    .map(lambda x: (x[0], x[1][1][0], x[1][1][1], x[1][0])) \\\n",
        "                    .sortBy(lambda x: -x[3])\n",
        "\n",
        "print(\"\\nRDD Result — Top songs with genre:\")\n",
        "for song_id, song_name, genre, plays in joined.collect():\n",
        "    print(f\"  {song_name} ({genre}): {plays} plays\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eioVtYTxNPW-",
        "outputId": "7778d3aa-cbf3-47e6-9786-c07de9d4c462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "PIPELINE 3: Top Songs with Genre (DataFrame)\n",
            "==================================================\n",
            "\n",
            "DataFrame Result — Top songs with genre:\n",
            "+-------+--------------+-----+----------+\n",
            "|song_id|     song_name|genre|play_count|\n",
            "+-------+--------------+-----+----------+\n",
            "|   S001|Midnight Drive|  Pop|         6|\n",
            "|   S002|   Ocean Waves| Rock|         3|\n",
            "|   S004|  Thunder Road| Rock|         2|\n",
            "|   S005|   Sunset Blvd| Jazz|         2|\n",
            "|   S003|   City Lights|  Pop|         2|\n",
            "+-------+--------------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 4b: DataFrame Version (Rewrite)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"PIPELINE 3: Top Songs with Genre (DataFrame)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "plays_df = spark.read.csv(\"plays.csv\", header=True, inferSchema=True)\n",
        "songs_df = spark.read.csv(\"songs.csv\", header=True, inferSchema=True)\n",
        "\n",
        "top_songs_df = (\n",
        "    plays_df\n",
        "    .groupBy(\"song_id\")\n",
        "    .agg(count(\"play_id\").alias(\"play_count\"))\n",
        "    .join(songs_df, \"song_id\")\n",
        "    .select(\"song_id\", \"song_name\", \"genre\", \"play_count\")\n",
        "    .orderBy(desc(\"play_count\"))\n",
        ")\n",
        "\n",
        "print(\"\\nDataFrame Result — Top songs with genre:\")\n",
        "top_songs_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn9RzhouNPTm",
        "outputId": "3e34e2b2-0101-48bc-bcd8-741094d6a9be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verification:\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "# Step 4c: Compare and verify\n",
        "\n",
        "rdd_result = [(r[0], r[3]) for r in joined.collect()]\n",
        "df_result = [(row[\"song_id\"], row[\"play_count\"]) for row in top_songs_df.collect()]\n",
        "\n",
        "rdd_sorted = sorted(rdd_result)\n",
        "df_sorted = sorted(df_result)\n",
        "\n",
        "print(\"\\nVerification:\")\n",
        "print(f\"Results match: {rdd_sorted == df_sorted}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO5fDJ-mPaGR"
      },
      "source": [
        "## Pipeline 3 — Top Songs with Genre (Join)\n",
        "\n",
        "| Metric | RDD Version | DataFrame Version |\n",
        "|--------|-------------|------------------|\n",
        "| Lines of code | 22 | 11 |\n",
        "| Join logic | Manual key-value join | Built-in join() |\n",
        "| Column access | Nested tuple access (x[1][1][0]) | Column names (\"song_name\") |\n",
        "| CSV parsing | Manual split(\",\") | Automatic (header=True) |\n",
        "| Aggregation | reduceByKey | groupBy + agg |\n",
        "| Optimization | None | Catalyst optimizer |\n",
        "| Readability | Very Low (deep tuple indexing) | Very High (SQL-like syntax) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLvwEIknPgv5"
      },
      "source": [
        "### Analysis\n",
        "\n",
        "- The DataFrame version reduces code by approximately 50%.\n",
        "- The RDD join requires complex nested tuple access (x[1][1][0]), which is difficult to read and maintain.\n",
        "- DataFrame joins are declarative and self-documenting.\n",
        "- Column-based operations reduce indexing errors.\n",
        "- Catalyst optimizer can optimize joins (broadcast joins, predicate pushdown), which RDDs cannot.\n",
        "- Join-heavy pipelines benefit the MOST from migrating to DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IYf1IXmPsul"
      },
      "source": [
        "## RDD to DataFrame Migration Guide\n",
        "\n",
        "### Overall Comparison\n",
        "\n",
        "| Metric | Pipeline 1 | Pipeline 2 | Pipeline 3 |\n",
        "|--------|------------|------------|------------|\n",
        "| RDD lines of code | 14 | 15 | 22 |\n",
        "| DataFrame lines of code | 8 | 9 | 11 |\n",
        "| Code reduction | 43% | 40% | 50% |\n",
        "| Results match | ✅ | ✅ | ✅ |\n",
        "\n",
        "---\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. DataFrames require approximately **40–50% less code** across all pipelines.\n",
        "2. Column access by name is **safer and more readable** than index-based access.\n",
        "3. Join operations are **dramatically more readable** with DataFrames.\n",
        "4. The **Catalyst optimizer** can optimize DataFrame queries but **cannot optimize RDD transformations**.\n",
        "5. Schema inference eliminates manual type conversion.\n",
        "6. DataFrame syntax is closer to SQL, making onboarding easier for new engineers.\n",
        "\n",
        "---\n",
        "\n",
        "### Technical Advantages of DataFrames\n",
        "\n",
        "| Feature | RDD | DataFrame |\n",
        "|----------|------|------------|\n",
        "| Schema awareness | ❌ No schema | ✅ Structured schema |\n",
        "| Catalyst optimizer | ❌ Not supported | ✅ Automatic optimization |\n",
        "| Tungsten execution engine | ❌ No | ✅ Yes |\n",
        "| Join optimization | ❌ Manual only | ✅ Broadcast + optimized joins |\n",
        "| SQL support | ❌ No | ✅ Yes |\n",
        "| Code maintainability | Low | High |\n",
        "\n",
        "---\n",
        "\n",
        "### Performance Considerations\n",
        "\n",
        "- RDD transformations operate at a lower abstraction level.\n",
        "- DataFrames allow Spark’s optimizer to:\n",
        "  - Reorder operations\n",
        "  - Push filters down\n",
        "  - Optimize joins\n",
        "  - Reduce shuffle\n",
        "- In real production systems, DataFrames are typically **2–5x faster** than equivalent RDD implementations.\n",
        "\n",
        "---\n",
        "\n",
        "### Migration Recommendation for StreamPulse\n",
        "\n",
        "**Priority 1 — Migrate Join-Based Pipelines**\n",
        "- Highest readability improvement\n",
        "- Highest performance gain\n",
        "- Most bug-prone in RDD form\n",
        "\n",
        "**Priority 2 — Migrate Aggregation Pipelines**\n",
        "- Significant code reduction\n",
        "- Performance optimization through Catalyst\n",
        "\n",
        "**Priority 3 — Keep RDD Only For:**\n",
        "- Low-level custom transformations\n",
        "- Complex unstructured data processing\n",
        "- Situations requiring fine-grained partition control\n",
        "\n",
        "---\n",
        "\n",
        "### Final Recommendation\n",
        "\n",
        "StreamPulse should standardize on the DataFrame API for all new development and gradually migrate legacy RDD pipelines.  \n",
        "\n",
        "RDD should be treated as a **low-level API for specialized use cases**, not the default choice.\n",
        "\n",
        "The migration improves:\n",
        "- Code readability\n",
        "- Maintainability\n",
        "- Performance\n",
        "- Team onboarding efficiency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI-kVrgsPgJ9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMA3NhF4PRaK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwPwSjQgPRWy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FixiLeSfPRTd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE6jUz4dPRQe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RhlelOmPRNJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
