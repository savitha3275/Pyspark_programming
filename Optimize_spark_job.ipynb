{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Scenario: StreamPulse Revenue Pipeline (Slow!)\n",
        "The StreamPulse revenue pipeline has been running for 45+ minutes in production. Management wants it under 10 minutes. The pipeline:\n",
        "\n",
        "Reads listening events, subscription data, and ad revenue\n",
        "Joins them into a unified revenue DataFrame\n",
        "Produces 6 reports: genre revenue, regional breakdown, subscription analysis, ad performance, artist payouts, and a daily summary\n",
        "The current code has multiple performance anti-patterns. Your job: fix them.\n",
        "\n"
      ],
      "metadata": {
        "id": "HiaapnZK3ZnL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDL66U5_3D6l",
        "outputId": "cc879b75-af48-417a-8e63-1eea554c96c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SparkSession created (intentionally misconfigured)\n"
          ]
        }
      ],
      "source": [
        "# Part 1: Set Up the \"Slow\" Environment\n",
        "\n",
        "# Intentionally misconfigured SparkSession:\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "import random\n",
        "import time\n",
        "\n",
        "# ANTI-PATTERN: broadcast disabled, too many shuffle partitions\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamPulse-Revenue-SLOW\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
        "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"‚úÖ SparkSession created (intentionally misconfigured)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the revenue dataset:\n",
        "\n",
        "import builtins\n",
        "import random\n",
        "\n",
        "N = 600000\n",
        "\n",
        "# Events (large)\n",
        "event_data = []\n",
        "for i in range(N):\n",
        "    event_data.append((\n",
        "        f\"EVT-{i+1:07d}\",\n",
        "        f\"USR-{random.randint(1, 100000):06d}\",\n",
        "        f\"ART-{random.randint(1, 5000):05d}\",\n",
        "        random.choice([\"Pop\", \"Rock\", \"Hip-Hop\", \"Jazz\", \"Electronic\", \"R&B\", \"Country\", \"Classical\"]),\n",
        "        random.choice([\"North America\", \"Europe\", \"Asia Pacific\", \"Latin America\", \"Africa\"]),\n",
        "        random.randint(15, 350),\n",
        "        random.choice([True, False]),\n",
        "        random.choice([\"mobile\", \"desktop\", \"smart_speaker\", \"tablet\", \"car\", \"tv\"]),\n",
        "        f\"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d}\",\n",
        "    ))\n",
        "\n",
        "events = spark.createDataFrame(event_data,\n",
        "    [\"event_id\", \"user_id\", \"artist_id\", \"genre\", \"region\",\n",
        "     \"duration_sec\", \"completed\", \"device\", \"event_date\"]) \\\n",
        "    .withColumn(\"event_date\", col(\"event_date\").cast(\"date\")) \\\n",
        "    .withColumn(\"month\", month(col(\"event_date\")))\n",
        "\n",
        "events.write.parquet(\"revenue_data/events\", mode=\"overwrite\")\n",
        "\n",
        "# Subscriptions (medium - 100K users with subscription info)\n",
        "sub_data = [(f\"USR-{i+1:06d}\",\n",
        "             random.choice([\"free\", \"individual\", \"family\", \"student\"]),\n",
        "             builtins.round(random.choice([0.0, 9.99, 14.99, 4.99]), 2),\n",
        "             random.choice([\"US\", \"UK\", \"DE\", \"JP\", \"BR\", \"IN\", \"KR\", \"FR\"]))\n",
        "            for i in range(100000)]\n",
        "subscriptions = spark.createDataFrame(sub_data, [\"user_id\", \"plan\", \"monthly_price\", \"country\"])\n",
        "subscriptions.write.parquet(\"revenue_data/subscriptions\", mode=\"overwrite\")\n",
        "\n",
        "# Ad rates (tiny - 8 genres x 6 devices = 48 rows)\n",
        "ad_data = []\n",
        "for genre in [\"Pop\", \"Rock\", \"Hip-Hop\", \"Jazz\", \"Electronic\", \"R&B\", \"Country\", \"Classical\"]:\n",
        "    for device in [\"mobile\", \"desktop\", \"smart_speaker\", \"tablet\", \"car\", \"tv\"]:\n",
        "        cpm = builtins.round(random.uniform(1.5, 8.0), 2)\n",
        "        ad_data.append((genre, device, cpm))\n",
        "ad_rates = spark.createDataFrame(ad_data, [\"ad_genre\", \"ad_device\", \"cpm\"])\n",
        "ad_rates.write.parquet(\"revenue_data/ad_rates\", mode=\"overwrite\")\n",
        "\n",
        "# Artist payout rates (small - 5000 artists)\n",
        "payout_data = [(f\"ART-{i+1:05d}\", builtins.round(random.uniform(0.003, 0.008), 4),\n",
        "                random.choice([\"major\", \"indie\", \"unsigned\"]))\n",
        "               for i in range(5000)]\n",
        "payouts = spark.createDataFrame(payout_data, [\"artist_id\", \"per_stream_rate\", \"label_type\"])\n",
        "payouts.write.parquet(\"revenue_data/payouts\", mode=\"overwrite\")\n",
        "\n",
        "# Reload from disk\n",
        "events = spark.read.parquet(\"revenue_data/events\")\n",
        "subscriptions = spark.read.parquet(\"revenue_data/subscriptions\")\n",
        "ad_rates = spark.read.parquet(\"revenue_data/ad_rates\")\n",
        "payouts = spark.read.parquet(\"revenue_data/payouts\")\n",
        "\n",
        "print(f\"Events: {events.count()} | Subs: {subscriptions.count()} | \"\n",
        "      f\"Ad rates: {ad_rates.count()} | Payouts: {payouts.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNdBAjb13qNQ",
        "outputId": "63f4f68e-1336-4381-d7f9-05729c4c18e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Events: 600000 | Subs: 100000 | Ad rates: 48 | Payouts: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: The Unoptimized Pipeline\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RUNNING UNOPTIMIZED PIPELINE (BASELINE)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "total_start = time.time()\n",
        "\n",
        "# Build enriched revenue DataFrame (NOT cached, recomputed every time)\n",
        "def build_revenue():\n",
        "    return events \\\n",
        "        .join(subscriptions, \"user_id\") \\\n",
        "        .join(ad_rates,\n",
        "              (events.genre == ad_rates.ad_genre) & (events.device == ad_rates.ad_device)) \\\n",
        "        .join(payouts, \"artist_id\") \\\n",
        "        .withColumn(\"ad_revenue\", col(\"cpm\") / 1000) \\\n",
        "        .withColumn(\"stream_payout\", col(\"per_stream_rate\")) \\\n",
        "        .withColumn(\"is_premium\", when(col(\"plan\") != \"free\", True).otherwise(False))\n",
        "\n",
        "# Report 1: Genre Revenue\n",
        "revenue = build_revenue()\n",
        "r1_start = time.time()\n",
        "report_1 = revenue.groupBy(\"genre\") \\\n",
        "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
        "         countDistinct(\"user_id\").alias(\"unique_listeners\")) \\\n",
        "    .collect()\n",
        "r1_time = time.time() - r1_start\n",
        "\n",
        "# Report 2: Regional Breakdown\n",
        "revenue = build_revenue()\n",
        "r2_start = time.time()\n",
        "report_2 = revenue.groupBy(\"region\", \"country\") \\\n",
        "    .agg(count(\"*\").alias(\"streams\"),\n",
        "         sum(\"ad_revenue\").alias(\"ad_rev\")) \\\n",
        "    .collect()\n",
        "r2_time = time.time() - r2_start\n",
        "\n",
        "# Report 3: Subscription Analysis\n",
        "revenue = build_revenue()\n",
        "r3_start = time.time()\n",
        "report_3 = revenue.groupBy(\"plan\") \\\n",
        "    .agg(countDistinct(\"user_id\").alias(\"users\"),\n",
        "         count(\"*\").alias(\"total_streams\"),\n",
        "         avg(\"duration_sec\").alias(\"avg_duration\")) \\\n",
        "    .collect()\n",
        "r3_time = time.time() - r3_start\n",
        "\n",
        "# Report 4: Ad Performance\n",
        "revenue = build_revenue()\n",
        "r4_start = time.time()\n",
        "report_4 = revenue.groupBy(\"device\", \"genre\") \\\n",
        "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
        "         count(\"*\").alias(\"impressions\")) \\\n",
        "    .collect()\n",
        "r4_time = time.time() - r4_start\n",
        "\n",
        "# Report 5: Artist Payouts\n",
        "revenue = build_revenue()\n",
        "r5_start = time.time()\n",
        "report_5 = revenue.groupBy(\"artist_id\", \"label_type\") \\\n",
        "    .agg(sum(\"stream_payout\").alias(\"total_payout\"),\n",
        "         count(\"*\").alias(\"total_streams\")) \\\n",
        "    .orderBy(desc(\"total_payout\")).limit(100) \\\n",
        "    .collect()\n",
        "r5_time = time.time() - r5_start\n",
        "\n",
        "# Report 6: Daily Summary\n",
        "revenue = build_revenue()\n",
        "r6_start = time.time()\n",
        "report_6 = revenue.groupBy(\"event_date\") \\\n",
        "    .agg(count(\"*\").alias(\"streams\"),\n",
        "         sum(\"ad_revenue\").alias(\"ad_rev\"),\n",
        "         countDistinct(\"user_id\").alias(\"unique_users\")) \\\n",
        "    .orderBy(\"event_date\") \\\n",
        "    .collect()\n",
        "r6_time = time.time() - r6_start\n",
        "\n",
        "baseline_total = time.time() - total_start\n",
        "\n",
        "print(f\"\\nReport 1 (genre):        {r1_time:.2f}s\")\n",
        "print(f\"Report 2 (regional):     {r2_time:.2f}s\")\n",
        "print(f\"Report 3 (subscription): {r3_time:.2f}s\")\n",
        "print(f\"Report 4 (ad perf):      {r4_time:.2f}s\")\n",
        "print(f\"Report 5 (payouts):      {r5_time:.2f}s\")\n",
        "print(f\"Report 6 (daily):        {r6_time:.2f}s\")\n",
        "print(f\"\\n‚è±Ô∏è  BASELINE TOTAL: {baseline_total:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8dIW8R632DF",
        "outputId": "43842576-18f5-4b8d-b84f-496eac4ab6f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "RUNNING UNOPTIMIZED PIPELINE (BASELINE)\n",
            "============================================================\n",
            "\n",
            "Report 1 (genre):        54.86s\n",
            "Report 2 (regional):     25.24s\n",
            "Report 3 (subscription): 32.94s\n",
            "Report 4 (ad perf):      21.38s\n",
            "Report 5 (payouts):      15.00s\n",
            "Report 6 (daily):        39.85s\n",
            "\n",
            "‚è±Ô∏è  BASELINE TOTAL: 190.12s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBASELINE PLAN:\")\n",
        "build_revenue().groupBy(\"genre\").agg(sum(\"ad_revenue\")).explain(mode=\"formatted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1mxnXzZ4w-v",
        "outputId": "4b9b422d-12dc-4661-acdc-e5b06007aada"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BASELINE PLAN:\n",
            "== Physical Plan ==\n",
            "* HashAggregate (33)\n",
            "+- Exchange (32)\n",
            "   +- * HashAggregate (31)\n",
            "      +- * Project (30)\n",
            "         +- * SortMergeJoin Inner (29)\n",
            "            :- * Sort (23)\n",
            "            :  +- Exchange (22)\n",
            "            :     +- * Project (21)\n",
            "            :        +- * SortMergeJoin Inner (20)\n",
            "            :           :- * Sort (14)\n",
            "            :           :  +- Exchange (13)\n",
            "            :           :     +- * Project (12)\n",
            "            :           :        +- * SortMergeJoin Inner (11)\n",
            "            :           :           :- * Sort (5)\n",
            "            :           :           :  +- Exchange (4)\n",
            "            :           :           :     +- * Filter (3)\n",
            "            :           :           :        +- * ColumnarToRow (2)\n",
            "            :           :           :           +- Scan parquet  (1)\n",
            "            :           :           +- * Sort (10)\n",
            "            :           :              +- Exchange (9)\n",
            "            :           :                 +- * Filter (8)\n",
            "            :           :                    +- * ColumnarToRow (7)\n",
            "            :           :                       +- Scan parquet  (6)\n",
            "            :           +- * Sort (19)\n",
            "            :              +- Exchange (18)\n",
            "            :                 +- * Filter (17)\n",
            "            :                    +- * ColumnarToRow (16)\n",
            "            :                       +- Scan parquet  (15)\n",
            "            +- * Sort (28)\n",
            "               +- Exchange (27)\n",
            "                  +- * Filter (26)\n",
            "                     +- * ColumnarToRow (25)\n",
            "                        +- Scan parquet  (24)\n",
            "\n",
            "\n",
            "(1) Scan parquet \n",
            "Output [4]: [user_id#33, artist_id#34, genre#35, device#39]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/revenue_data/events]\n",
            "PushedFilters: [IsNotNull(user_id), IsNotNull(genre), IsNotNull(device), IsNotNull(artist_id)]\n",
            "ReadSchema: struct<user_id:string,artist_id:string,genre:string,device:string>\n",
            "\n",
            "(2) ColumnarToRow [codegen id : 1]\n",
            "Input [4]: [user_id#33, artist_id#34, genre#35, device#39]\n",
            "\n",
            "(3) Filter [codegen id : 1]\n",
            "Input [4]: [user_id#33, artist_id#34, genre#35, device#39]\n",
            "Condition : (((isnotnull(user_id#33) AND isnotnull(genre#35)) AND isnotnull(device#39)) AND isnotnull(artist_id#34))\n",
            "\n",
            "(4) Exchange\n",
            "Input [4]: [user_id#33, artist_id#34, genre#35, device#39]\n",
            "Arguments: hashpartitioning(user_id#33, 200), ENSURE_REQUIREMENTS, [plan_id=2053]\n",
            "\n",
            "(5) Sort [codegen id : 2]\n",
            "Input [4]: [user_id#33, artist_id#34, genre#35, device#39]\n",
            "Arguments: [user_id#33 ASC NULLS FIRST], false, 0\n",
            "\n",
            "(6) Scan parquet \n",
            "Output [1]: [user_id#42]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/revenue_data/subscriptions]\n",
            "PushedFilters: [IsNotNull(user_id)]\n",
            "ReadSchema: struct<user_id:string>\n",
            "\n",
            "(7) ColumnarToRow [codegen id : 3]\n",
            "Input [1]: [user_id#42]\n",
            "\n",
            "(8) Filter [codegen id : 3]\n",
            "Input [1]: [user_id#42]\n",
            "Condition : isnotnull(user_id#42)\n",
            "\n",
            "(9) Exchange\n",
            "Input [1]: [user_id#42]\n",
            "Arguments: hashpartitioning(user_id#42, 200), ENSURE_REQUIREMENTS, [plan_id=2062]\n",
            "\n",
            "(10) Sort [codegen id : 4]\n",
            "Input [1]: [user_id#42]\n",
            "Arguments: [user_id#42 ASC NULLS FIRST], false, 0\n",
            "\n",
            "(11) SortMergeJoin [codegen id : 5]\n",
            "Left keys [1]: [user_id#33]\n",
            "Right keys [1]: [user_id#42]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(12) Project [codegen id : 5]\n",
            "Output [3]: [artist_id#34, genre#35, device#39]\n",
            "Input [5]: [user_id#33, artist_id#34, genre#35, device#39, user_id#42]\n",
            "\n",
            "(13) Exchange\n",
            "Input [3]: [artist_id#34, genre#35, device#39]\n",
            "Arguments: hashpartitioning(genre#35, device#39, 200), ENSURE_REQUIREMENTS, [plan_id=2070]\n",
            "\n",
            "(14) Sort [codegen id : 6]\n",
            "Input [3]: [artist_id#34, genre#35, device#39]\n",
            "Arguments: [genre#35 ASC NULLS FIRST, device#39 ASC NULLS FIRST], false, 0\n",
            "\n",
            "(15) Scan parquet \n",
            "Output [3]: [ad_genre#46, ad_device#47, cpm#48]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/revenue_data/ad_rates]\n",
            "PushedFilters: [IsNotNull(ad_genre), IsNotNull(ad_device)]\n",
            "ReadSchema: struct<ad_genre:string,ad_device:string,cpm:double>\n",
            "\n",
            "(16) ColumnarToRow [codegen id : 7]\n",
            "Input [3]: [ad_genre#46, ad_device#47, cpm#48]\n",
            "\n",
            "(17) Filter [codegen id : 7]\n",
            "Input [3]: [ad_genre#46, ad_device#47, cpm#48]\n",
            "Condition : (isnotnull(ad_genre#46) AND isnotnull(ad_device#47))\n",
            "\n",
            "(18) Exchange\n",
            "Input [3]: [ad_genre#46, ad_device#47, cpm#48]\n",
            "Arguments: hashpartitioning(ad_genre#46, ad_device#47, 200), ENSURE_REQUIREMENTS, [plan_id=2079]\n",
            "\n",
            "(19) Sort [codegen id : 8]\n",
            "Input [3]: [ad_genre#46, ad_device#47, cpm#48]\n",
            "Arguments: [ad_genre#46 ASC NULLS FIRST, ad_device#47 ASC NULLS FIRST], false, 0\n",
            "\n",
            "(20) SortMergeJoin [codegen id : 9]\n",
            "Left keys [2]: [genre#35, device#39]\n",
            "Right keys [2]: [ad_genre#46, ad_device#47]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(21) Project [codegen id : 9]\n",
            "Output [3]: [artist_id#34, genre#35, cpm#48]\n",
            "Input [6]: [artist_id#34, genre#35, device#39, ad_genre#46, ad_device#47, cpm#48]\n",
            "\n",
            "(22) Exchange\n",
            "Input [3]: [artist_id#34, genre#35, cpm#48]\n",
            "Arguments: hashpartitioning(artist_id#34, 200), ENSURE_REQUIREMENTS, [plan_id=2087]\n",
            "\n",
            "(23) Sort [codegen id : 10]\n",
            "Input [3]: [artist_id#34, genre#35, cpm#48]\n",
            "Arguments: [artist_id#34 ASC NULLS FIRST], false, 0\n",
            "\n",
            "(24) Scan parquet \n",
            "Output [1]: [artist_id#49]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/revenue_data/payouts]\n",
            "PushedFilters: [IsNotNull(artist_id)]\n",
            "ReadSchema: struct<artist_id:string>\n",
            "\n",
            "(25) ColumnarToRow [codegen id : 11]\n",
            "Input [1]: [artist_id#49]\n",
            "\n",
            "(26) Filter [codegen id : 11]\n",
            "Input [1]: [artist_id#49]\n",
            "Condition : isnotnull(artist_id#49)\n",
            "\n",
            "(27) Exchange\n",
            "Input [1]: [artist_id#49]\n",
            "Arguments: hashpartitioning(artist_id#49, 200), ENSURE_REQUIREMENTS, [plan_id=2096]\n",
            "\n",
            "(28) Sort [codegen id : 12]\n",
            "Input [1]: [artist_id#49]\n",
            "Arguments: [artist_id#49 ASC NULLS FIRST], false, 0\n",
            "\n",
            "(29) SortMergeJoin [codegen id : 13]\n",
            "Left keys [1]: [artist_id#34]\n",
            "Right keys [1]: [artist_id#49]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(30) Project [codegen id : 13]\n",
            "Output [2]: [genre#35, (cpm#48 / 1000.0) AS ad_revenue#332]\n",
            "Input [4]: [artist_id#34, genre#35, cpm#48, artist_id#49]\n",
            "\n",
            "(31) HashAggregate [codegen id : 13]\n",
            "Input [2]: [genre#35, ad_revenue#332]\n",
            "Keys [1]: [genre#35]\n",
            "Functions [1]: [partial_sum(ad_revenue#332)]\n",
            "Aggregate Attributes [1]: [sum#358]\n",
            "Results [2]: [genre#35, sum#359]\n",
            "\n",
            "(32) Exchange\n",
            "Input [2]: [genre#35, sum#359]\n",
            "Arguments: hashpartitioning(genre#35, 200), ENSURE_REQUIREMENTS, [plan_id=2105]\n",
            "\n",
            "(33) HashAggregate [codegen id : 14]\n",
            "Input [2]: [genre#35, sum#359]\n",
            "Keys [1]: [genre#35]\n",
            "Functions [1]: [sum(ad_revenue#332)]\n",
            "Aggregate Attributes [1]: [sum(ad_revenue#332)#356]\n",
            "Results [2]: [genre#35, sum(ad_revenue#332)#356 AS sum(ad_revenue)#357]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anti-Pattern Documentation Table\n",
        "\n",
        "| Anti-Pattern | Description                                            | Impact                                                          |\n",
        "| ------------ | ------------------------------------------------------ | --------------------------------------------------------------- |\n",
        "| 1            | Broadcast disabled (`autoBroadcastJoinThreshold = -1`) | Forces SortMergeJoin even for tiny tables ‚Üí unnecessary shuffle |\n",
        "| 2            | `build_revenue()` called 6 times                       | Entire join pipeline recomputed 6 times                         |\n",
        "| 3            | No caching of enriched DataFrame                       | Full lineage re-executed for every report                       |\n",
        "| 4            | Too many shuffle partitions (200 in local mode)        | Excessive task overhead and scheduling cost                     |\n",
        "| 5            | No column pruning before joins                         | Unnecessary data read and shuffled across cluster               |\n",
        "\n"
      ],
      "metadata": {
        "id": "LeINPWnu6BjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimization 1 :OPTIMIZATION 1 ‚Äî Enable Broadcast Joins\n",
        "# What we fix\n",
        "\n",
        "# Small tables (ad_rates, payouts) should use BroadcastHashJoin instead of SortMergeJoin.\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10MB\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "def build_revenue_opt1():\n",
        "    return events \\\n",
        "        .join(subscriptions, \"user_id\") \\\n",
        "        .join(\n",
        "            broadcast(ad_rates),\n",
        "            (events.genre == ad_rates.ad_genre) &\n",
        "            (events.device == ad_rates.ad_device)\n",
        "        ) \\\n",
        "        .join(\n",
        "            broadcast(payouts),\n",
        "            \"artist_id\"\n",
        "        ) \\\n",
        "        .withColumn(\"ad_revenue\", col(\"cpm\") / 1000) \\\n",
        "        .withColumn(\"stream_payout\", col(\"per_stream_rate\")) \\\n",
        "        .withColumn(\"is_premium\", when(col(\"plan\") != \"free\", True).otherwise(False))\n",
        "\n",
        "start = time.time()\n",
        "build_revenue_opt1().groupBy(\"genre\").agg(sum(\"ad_revenue\")).collect()\n",
        "print(\"Optimization 1 time:\", time.time() - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73xDh3fJ6FsF",
        "outputId": "72e4947c-14fa-446e-fec2-e69901b267cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization 1 time: 5.916804552078247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OPTIMIZATION 2 ‚Äî Cache Enriched DataFrame\n",
        "#What we fix\n",
        "\n",
        "#Baseline rebuilt full join 6 times.\n",
        "\n",
        "#Code\n",
        "revenue_cached = build_revenue_opt1().cache()\n",
        "\n",
        "# Materialize cache\n",
        "revenue_cached.count()\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "revenue_cached.groupBy(\"genre\").agg(sum(\"ad_revenue\")).collect()\n",
        "revenue_cached.groupBy(\"region\", \"country\").agg(count(\"*\")).collect()\n",
        "revenue_cached.groupBy(\"plan\").agg(count(\"*\")).collect()\n",
        "revenue_cached.groupBy(\"device\", \"genre\").agg(count(\"*\")).collect()\n",
        "revenue_cached.groupBy(\"artist_id\").agg(sum(\"stream_payout\")).collect()\n",
        "revenue_cached.groupBy(\"event_date\").agg(count(\"*\")).collect()\n",
        "\n",
        "print(\"Optimization 2 time:\", time.time() - start)\n",
        "\n",
        "revenue_cached.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBm-DsXc9n78",
        "outputId": "268db1db-fdb6-4de4-9864-df7970435dff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization 2 time: 14.344804763793945\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[artist_id: string, user_id: string, event_id: string, genre: string, region: string, duration_sec: bigint, completed: boolean, device: string, event_date: date, month: int, plan: string, monthly_price: double, country: string, ad_genre: string, ad_device: string, cpm: double, per_stream_rate: double, label_type: string, ad_revenue: double, stream_payout: double, is_premium: boolean]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OPTIMIZATION 3 ‚Äî Reduce Shuffle Partitions\n",
        "# What we fix\n",
        "\n",
        "# 200 partitions is too high for local mode.\n",
        "\n",
        "# Config\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "#Re-run\n",
        "revenue_opt3 = build_revenue_opt1().cache()\n",
        "revenue_opt3.count()\n",
        "\n",
        "start = time.time()\n",
        "revenue_opt3.groupBy(\"genre\").agg(sum(\"ad_revenue\")).collect()\n",
        "print(\"Optimization 3 time:\", time.time() - start)\n",
        "\n",
        "revenue_opt3.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e2_tfta90bf",
        "outputId": "9051fc16-c866-449c-9067-75b303a5957b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization 3 time: 0.5991461277008057\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[artist_id: string, user_id: string, event_id: string, genre: string, region: string, duration_sec: bigint, completed: boolean, device: string, event_date: date, month: int, plan: string, monthly_price: double, country: string, ad_genre: string, ad_device: string, cpm: double, per_stream_rate: double, label_type: string, ad_revenue: double, stream_payout: double, is_premium: boolean]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OPTIMIZATION 4 ‚Äî Column Pruning\n",
        "# What we fix\n",
        "\n",
        "# Baseline read unnecessary columns.\n",
        "\n",
        "#Code\n",
        "events_small = events.select(\n",
        "    \"event_id\", \"user_id\", \"artist_id\",\n",
        "    \"genre\", \"region\", \"duration_sec\",\n",
        "    \"device\", \"event_date\"\n",
        ")\n",
        "\n",
        "subscriptions_small = subscriptions.select(\n",
        "    \"user_id\", \"plan\", \"country\"\n",
        ")\n",
        "\n",
        "ad_rates_small = ad_rates.select(\n",
        "    \"ad_genre\", \"ad_device\", \"cpm\"\n",
        ")\n",
        "\n",
        "payouts_small = payouts.select(\n",
        "    \"artist_id\", \"per_stream_rate\", \"label_type\"\n",
        ")\n",
        "\n",
        "revenue_opt4 = events_small \\\n",
        "    .join(subscriptions_small, \"user_id\") \\\n",
        "    .join(\n",
        "        broadcast(ad_rates_small),\n",
        "        (col(\"genre\") == col(\"ad_genre\")) &\n",
        "        (col(\"device\") == col(\"ad_device\"))\n",
        "    ) \\\n",
        "    .join(\n",
        "        broadcast(payouts_small),\n",
        "        \"artist_id\"\n",
        "    ) \\\n",
        "    .withColumn(\"ad_revenue\", col(\"cpm\") / 1000) \\\n",
        "    .withColumn(\"stream_payout\", col(\"per_stream_rate\")) \\\n",
        "    .drop(\"ad_genre\", \"ad_device\") \\\n",
        "    .cache()\n",
        "\n",
        "revenue_opt4.count()\n",
        "\n",
        "start = time.time()\n",
        "revenue_opt4.groupBy(\"genre\").agg(sum(\"ad_revenue\")).collect()\n",
        "print(\"Optimization 4 time:\", time.time() - start)\n",
        "\n",
        "revenue_opt4.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2imIERJc9_EH",
        "outputId": "6cc24602-104c-4ef1-dc74-7b63cd3d4fe7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization 4 time: 0.20737957954406738\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[artist_id: string, user_id: string, event_id: string, genre: string, region: string, duration_sec: bigint, device: string, event_date: date, plan: string, country: string, cpm: double, per_stream_rate: double, label_type: string, ad_revenue: double, stream_payout: double]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OPTIMIZATION 5 ‚Äî Filter Early\n",
        "# What we fix\n",
        "\n",
        "#If reports only need certain months, filter BEFORE joins.\n",
        "\n",
        "# Code Example (Q1 only)\n",
        "events_filtered = events.filter(col(\"month\").isin([1, 2, 3]))\n",
        "\n",
        "revenue_opt5 = events_filtered \\\n",
        "    .join(subscriptions_small, \"user_id\") \\\n",
        "    .join(\n",
        "        broadcast(ad_rates_small),\n",
        "        (col(\"genre\") == col(\"ad_genre\")) &\n",
        "        (col(\"device\") == col(\"ad_device\"))\n",
        "    ) \\\n",
        "    .join(\n",
        "        broadcast(payouts_small),\n",
        "        \"artist_id\"\n",
        "    ) \\\n",
        "    .withColumn(\"ad_revenue\", col(\"cpm\") / 1000) \\\n",
        "    .withColumn(\"stream_payout\", col(\"per_stream_rate\")) \\\n",
        "    .cache()\n",
        "\n",
        "revenue_opt5.count()\n",
        "\n",
        "start = time.time()\n",
        "revenue_opt5.groupBy(\"genre\").agg(sum(\"ad_revenue\")).collect()\n",
        "print(\"Optimization 5 time:\", time.time() - start)\n",
        "\n",
        "revenue_opt5.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mH7czU9-IIT",
        "outputId": "e8865f8b-824b-4449-f0c2-936673886ee8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization 5 time: 0.15427231788635254\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[artist_id: string, user_id: string, event_id: string, genre: string, region: string, duration_sec: bigint, completed: boolean, device: string, event_date: date, month: int, plan: string, country: string, ad_genre: string, ad_device: string, cpm: double, per_stream_rate: double, label_type: string, ad_revenue: double, stream_payout: double]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"RUNNING FULLY OPTIMIZED PIPELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Reset config\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "\n",
        "total_start = time.time()\n",
        "\n",
        "# Build enriched DataFrame ONCE with all optimizations\n",
        "revenue_opt = events \\\n",
        "    .select(\"event_id\", \"user_id\", \"artist_id\", \"genre\", \"region\",\n",
        "            \"duration_sec\", \"completed\", \"device\", \"event_date\", \"month\") \\\n",
        "    .join(subscriptions.select(\"user_id\", \"plan\", \"country\"), \"user_id\") \\\n",
        "    .join(broadcast(ad_rates),\n",
        "          (col(\"genre\") == col(\"ad_genre\")) & (col(\"device\") == col(\"ad_device\"))) \\\n",
        "    .join(broadcast(payouts), \"artist_id\") \\\n",
        "    .withColumn(\"ad_revenue\", col(\"cpm\") / 1000) \\\n",
        "    .withColumn(\"stream_payout\", col(\"per_stream_rate\")) \\\n",
        "    .drop(\"ad_genre\", \"ad_device\")\n",
        "\n",
        "# Cache the shared DataFrame\n",
        "revenue_opt.cache()\n",
        "cache_start = time.time()\n",
        "row_count = revenue_opt.count()\n",
        "cache_time = time.time() - cache_start\n",
        "print(f\"‚úÖ Cached {row_count} rows in {cache_time:.2f}s\")\n",
        "\n",
        "# Run all 6 reports from cache\n",
        "r1 = revenue_opt.groupBy(\"genre\").agg(sum(\"ad_revenue\"), countDistinct(\"user_id\")).collect()\n",
        "r2 = revenue_opt.groupBy(\"region\", \"country\").agg(count(\"*\"), sum(\"ad_revenue\")).collect()\n",
        "r3 = revenue_opt.groupBy(\"plan\").agg(countDistinct(\"user_id\"), count(\"*\"), avg(\"duration_sec\")).collect()\n",
        "r4 = revenue_opt.groupBy(\"device\", \"genre\").agg(sum(\"ad_revenue\"), count(\"*\")).collect()\n",
        "r5 = revenue_opt.groupBy(\"artist_id\", \"label_type\") \\\n",
        "    .agg(sum(\"stream_payout\"), count(\"*\")) \\\n",
        "    .orderBy(desc(\"sum(stream_payout)\")).limit(100).collect()\n",
        "r6 = revenue_opt.groupBy(\"event_date\") \\\n",
        "    .agg(count(\"*\"), sum(\"ad_revenue\"), countDistinct(\"user_id\")) \\\n",
        "    .orderBy(\"event_date\").collect()\n",
        "\n",
        "optimized_total = time.time() - total_start\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  OPTIMIZED TOTAL: {optimized_total:.2f}s\")\n",
        "print(f\"‚è±Ô∏è  BASELINE TOTAL:  {baseline_total:.2f}s\")\n",
        "print(f\"üìà SPEEDUP:          {baseline_total/optimized_total:.1f}x\")\n",
        "print(f\"üìâ TIME SAVED:       {baseline_total - optimized_total:.2f}s ({(1-optimized_total/baseline_total)*100:.0f}%)\")\n",
        "\n",
        "# Verify the plan\n",
        "print(\"\\nOPTIMIZED PLAN:\")\n",
        "revenue_opt.groupBy(\"genre\").agg(sum(\"ad_revenue\")).explain(mode=\"formatted\")\n",
        "\n",
        "revenue_opt.unpersist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu2cSu2C-UKE",
        "outputId": "314e87a5-9709-47cc-a957-b6a0cce656ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "RUNNING FULLY OPTIMIZED PIPELINE\n",
            "============================================================\n",
            "‚úÖ Cached 600000 rows in 7.88s\n",
            "\n",
            "‚è±Ô∏è  OPTIMIZED TOTAL: 19.98s\n",
            "‚è±Ô∏è  BASELINE TOTAL:  190.12s\n",
            "üìà SPEEDUP:          9.5x\n",
            "üìâ TIME SAVED:       170.15s (89%)\n",
            "\n",
            "OPTIMIZED PLAN:\n",
            "== Physical Plan ==\n",
            "* HashAggregate (26)\n",
            "+- Exchange (25)\n",
            "   +- * HashAggregate (24)\n",
            "      +- InMemoryTableScan (1)\n",
            "            +- InMemoryRelation (2)\n",
            "                  +- * Project (23)\n",
            "                     +- * BroadcastHashJoin Inner BuildRight (22)\n",
            "                        :- * Project (17)\n",
            "                        :  +- * BroadcastHashJoin Inner BuildRight (16)\n",
            "                        :     :- * Project (11)\n",
            "                        :     :  +- * BroadcastHashJoin Inner BuildRight (10)\n",
            "                        :     :     :- * Filter (5)\n",
            "                        :     :     :  +- * ColumnarToRow (4)\n",
            "                        :     :     :     +- Scan parquet  (3)\n",
            "                        :     :     +- BroadcastExchange (9)\n",
            "                        :     :        +- * Filter (8)\n",
            "                        :     :           +- * ColumnarToRow (7)\n",
            "                        :     :              +- Scan parquet  (6)\n",
            "                        :     +- BroadcastExchange (15)\n",
            "                        :        +- * Filter (14)\n",
            "                        :           +- * ColumnarToRow (13)\n",
            "                        :              +- Scan parquet  (12)\n",
            "                        +- BroadcastExchange (21)\n",
            "                           +- * Filter (20)\n",
            "                              +- * ColumnarToRow (19)\n",
            "                                 +- Scan parquet  (18)\n",
            "\n",
            "\n",
            "(1) InMemoryTableScan\n",
            "Output [2]: [genre#35, ad_revenue#7519]\n",
            "Arguments: [genre#35, ad_revenue#7519]\n",
            "\n",
            "(2) InMemoryRelation\n",
            "Arguments: [artist_id#34, user_id#33, event_id#32, genre#35, region#36, duration_sec#37L, completed#38, device#39, event_date#40, month#41, plan#43, country#45, cpm#48, per_stream_rate#50, label_type#51, ad_revenue#7519, stream_payout#7520], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "\n",
            "(3) Scan parquet \n",
            "Output [10]: [event_id#32, user_id#33, artist_id#34, genre#35, region#36, duration_sec#37L, completed#38, device#39, event_date#40, month#41]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/revenue_data/events]\n",
            "PushedFilters: [IsNotNull(user_id), IsNotNull(genre), IsNotNull(device), IsNotNull(artist_id)]\n",
            "ReadSchema: struct<event_id:string,user_id:string,artist_id:string,genre:string,region:string,duration_sec:bigint,completed:boolean,device:string,event_date:date,month:int>\n",
            "\n",
            "(4) ColumnarToRow [codegen id : 4]\n",
            "Input [10]: [event_id#32, user_id#33, artist_id#34, genre#35, region#36, duration_sec#37L, completed#38, device#39, event_date#40, month#41]\n",
            "\n",
            "(5) Filter [codegen id : 4]\n",
            "Input [10]: [event_id#32, user_id#33, artist_id#34, genre#35, region#36, duration_sec#37L, completed#38, device#39, event_date#40, month#41]\n",
            "Condition : (((isnotnull(user_id#33) AND isnotnull(genre#35)) AND isnotnull(device#39)) AND isnotnull(artist_id#34))\n",
            "\n",
            "(6) Scan parquet \n",
            "Output [3]: [user_id#42, plan#43, country#45]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/revenue_data/subscriptions]\n",
            "PushedFilters: [IsNotNull(user_id)]\n",
            "ReadSchema: struct<user_id:string,plan:string,country:string>\n",
            "\n",
            "(7) ColumnarToRow [codegen id : 1]\n",
            "Input [3]: [user_id#42, plan#43, country#45]\n",
            "\n",
            "(8) Filter [codegen id : 1]\n",
            "Input [3]: [user_id#42, plan#43, country#45]\n",
            "Condition : isnotnull(user_id#42)\n",
            "\n",
            "(9) BroadcastExchange\n",
            "Input [3]: [user_id#42, plan#43, country#45]\n",
            "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=3336]\n",
            "\n",
            "(10) BroadcastHashJoin [codegen id : 4]\n",
            "Left keys [1]: [user_id#33]\n",
            "Right keys [1]: [user_id#42]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(11) Project [codegen id : 4]\n",
            "Output [12]: [user_id#33, event_id#32, artist_id#34, genre#35, region#36, duration_sec#37L, completed#38, device#39, event_date#40, month#41, plan#43, country#45]\n",
            "Input [13]: [event_id#32, user_id#33, artist_id#34, genre#35, region#36, duration_sec#37L, completed#38, device#39, event_date#40, month#41, user_id#42, plan#43, country#45]\n",
            "\n",
            "(12) Scan parquet \n",
            "Output [3]: [ad_genre#46, ad_device#47, cpm#48]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/revenue_data/ad_rates]\n",
            "PushedFilters: [IsNotNull(ad_genre), IsNotNull(ad_device)]\n",
            "ReadSchema: struct<ad_genre:string,ad_device:string,cpm:double>\n",
            "\n",
            "(13) ColumnarToRow [codegen id : 2]\n",
            "Input [3]: [ad_genre#46, ad_device#47, cpm#48]\n",
            "\n",
            "(14) Filter [codegen id : 2]\n",
            "Input [3]: [ad_genre#46, ad_device#47, cpm#48]\n",
            "Condition : (isnotnull(ad_genre#46) AND isnotnull(ad_device#47))\n",
            "\n",
            "(15) BroadcastExchange\n",
            "Input [3]: [ad_genre#46, ad_device#47, cpm#48]\n",
            "Arguments: HashedRelationBroadcastMode(List(input[0, string, false], input[1, string, false]),false), [plan_id=3344]\n",
            "\n",
            "(16) BroadcastHashJoin [codegen id : 4]\n",
            "Left keys [2]: [genre#35, device#39]\n",
            "Right keys [2]: [ad_genre#46, ad_device#47]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(17) Project [codegen id : 4]\n",
            "Output [13]: [user_id#33, event_id#32, artist_id#34, genre#35, region#36, duration_sec#37L, completed#38, device#39, event_date#40, month#41, plan#43, country#45, cpm#48]\n",
            "Input [15]: [user_id#33, event_id#32, artist_id#34, genre#35, region#36, duration_sec#37L, completed#38, device#39, event_date#40, month#41, plan#43, country#45, ad_genre#46, ad_device#47, cpm#48]\n",
            "\n",
            "(18) Scan parquet \n",
            "Output [3]: [artist_id#49, per_stream_rate#50, label_type#51]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/revenue_data/payouts]\n",
            "PushedFilters: [IsNotNull(artist_id)]\n",
            "ReadSchema: struct<artist_id:string,per_stream_rate:double,label_type:string>\n",
            "\n",
            "(19) ColumnarToRow [codegen id : 3]\n",
            "Input [3]: [artist_id#49, per_stream_rate#50, label_type#51]\n",
            "\n",
            "(20) Filter [codegen id : 3]\n",
            "Input [3]: [artist_id#49, per_stream_rate#50, label_type#51]\n",
            "Condition : isnotnull(artist_id#49)\n",
            "\n",
            "(21) BroadcastExchange\n",
            "Input [3]: [artist_id#49, per_stream_rate#50, label_type#51]\n",
            "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=3352]\n",
            "\n",
            "(22) BroadcastHashJoin [codegen id : 4]\n",
            "Left keys [1]: [artist_id#34]\n",
            "Right keys [1]: [artist_id#49]\n",
            "Join type: Inner\n",
            "Join condition: None\n",
            "\n",
            "(23) Project [codegen id : 4]\n",
            "Output [17]: [artist_id#34, user_id#33, event_id#32, genre#35, region#36, duration_sec#37L, completed#38, device#39, event_date#40, month#41, plan#43, country#45, cpm#48, per_stream_rate#50, label_type#51, (cpm#48 / 1000.0) AS ad_revenue#7519, per_stream_rate#50 AS stream_payout#7520]\n",
            "Input [16]: [user_id#33, event_id#32, artist_id#34, genre#35, region#36, duration_sec#37L, completed#38, device#39, event_date#40, month#41, plan#43, country#45, cpm#48, artist_id#49, per_stream_rate#50, label_type#51]\n",
            "\n",
            "(24) HashAggregate [codegen id : 1]\n",
            "Input [2]: [genre#35, ad_revenue#7519]\n",
            "Keys [1]: [genre#35]\n",
            "Functions [1]: [partial_sum(ad_revenue#7519)]\n",
            "Aggregate Attributes [1]: [sum#11072]\n",
            "Results [2]: [genre#35, sum#11073]\n",
            "\n",
            "(25) Exchange\n",
            "Input [2]: [genre#35, sum#11073]\n",
            "Arguments: hashpartitioning(genre#35, 8), ENSURE_REQUIREMENTS, [plan_id=3665]\n",
            "\n",
            "(26) HashAggregate [codegen id : 2]\n",
            "Input [2]: [genre#35, sum#11073]\n",
            "Keys [1]: [genre#35]\n",
            "Functions [1]: [sum(ad_revenue#7519)]\n",
            "Aggregate Attributes [1]: [sum(ad_revenue#7519)#10815]\n",
            "Results [2]: [genre#35, sum(ad_revenue#7519)#10815 AS sum(ad_revenue)#10816]\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[artist_id: string, user_id: string, event_id: string, genre: string, region: string, duration_sec: bigint, completed: boolean, device: string, event_date: date, month: int, plan: string, country: string, cpm: double, per_stream_rate: double, label_type: string, ad_revenue: double, stream_payout: double]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execution Plan Improvements\n",
        "\n",
        "SortMergeJoin ‚Üí BroadcastHashJoin\n",
        "\n",
        "6 full recomputations ‚Üí 1 cached computation\n",
        "\n",
        "200 shuffle partitions ‚Üí 8\n",
        "\n",
        "Reduced input schema size\n",
        "\n",
        "Reduced shuffle spill\n",
        "\n",
        "Performance Principles Demonstrated\n",
        "\n",
        "Broadcast small dimension tables\n",
        "\n",
        "Cache reused transformations\n",
        "\n",
        "Reduce shuffle partitions to match cluster size\n",
        "\n",
        "Prune unused columns\n",
        "\n",
        "Filter early to reduce join size\n",
        "\n",
        "Avoid recomputation of expensive joins"
      ],
      "metadata": {
        "id": "KB3ecIF0-Ubv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a final optimization report:\n",
        "\n",
        "\n",
        "print(\"=\" * 65)\n",
        "print(\"OPTIMIZATION REPORT ‚Äî StreamPulse Revenue Pipeline\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "print(f\"\"\"\n",
        "Pipeline: Revenue Analytics (6 reports from joined data)\n",
        "\n",
        "CONFIGURATION CHANGES:\n",
        "  spark.sql.autoBroadcastJoinThreshold: -1 ‚Üí 10MB\n",
        "  spark.sql.shuffle.partitions: 200 ‚Üí 8\n",
        "  spark.sql.adaptive.enabled: false ‚Üí (unchanged for testing)\n",
        "\n",
        "CODE CHANGES:\n",
        "  1. broadcast() on ad_rates (48 rows) and payouts (5K rows)\n",
        "  2. .cache() on enriched DataFrame (built once, used 6 times)\n",
        "  3. Column pruning on all source tables\n",
        "  4. Single build_revenue() call instead of 6 separate calls\n",
        "\n",
        "RESULTS:\n",
        "  Baseline:  {baseline_total:.2f}s\n",
        "  Optimized: {optimized_total:.2f}s\n",
        "  Speedup:   {baseline_total/optimized_total:.1f}x\n",
        "\n",
        "PLAN IMPROVEMENTS:\n",
        "  - SortMergeJoin ‚Üí BroadcastHashJoin (ad_rates, payouts)\n",
        "  - 6 full recomputations ‚Üí 1 computation + 5 cache reads\n",
        "  - 200 shuffle partitions ‚Üí 8 (matched to local cores)\n",
        "  - ReadSchema reduced (column pruning)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "iW4Xv4sw-dUy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TYF1yEbY-ko1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}